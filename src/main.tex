% Classe.
\documentclass[
  %% Opções da classe memoir.
  article, % Indica que é um artigo acadêmico.
  11pt,    % Tamanho da fonte.
  extrafontsizes, % Carrega os pacotes fontenv e lmodern automaticamente.
  oneside, % Para impressão apenas no recto.
  a4paper, % Tamanho do papel.
  %% Opções da classe abntex2.
  %%% Tìtulos de letra maiúscula.
  chapter=TITLE,
  section=TITLE,
  subsection=TITLE,
  subsubsection=TITLE,
  %% Opções do pacote babel.
  english,
  brazilian
]{abntex2}
% Pacotes.
\usepackage{amsmath, amsthm, amssymb, mathtools} % Coisas de matemática.
\usepackage{microtype} % Personalização de justificação.
\usepackage{indentfirst} % Indenta o primeiro parágrafo de cada seção.
\usepackage{graphicx} % Coisas gráficas.
%% Citação.
\usepackage{csquotes} % Aspas que se ajustam ao contexto automaticamente.
\usepackage[
style=abnt,
backend=biber, % Motor de processamento mais moderno que o bibtex.
giveninits=true, % Abrevia os primeiros nomes.
uniquename=init, % Evita nomes ambiguos.
backref=true, % Indice as páginas onde as citações aparecem.
citecount=true % Contador de quantas vezes uma referência foi citada.
]{biblatex}
\addbibresource{bibliografia.bib}
% Comandos.
%% Imprime logo da UFS antes de imprimir a capa.
\newcommand{\imprimircapaelogo}{
  \begin{center}
    \includegraphics[scale=0.2]{img/logo_ufs}
  \end{center}
  \imprimircapa
}
%% Cria uma badge e link direcionando ao ORCID da pessoa.
%% Parâmetros:
%% #1: Link para o ORCID da pessoa.
%% #2: Nome da pessoa.
%% Exemplo:
%% \orcid{https://orcid.org/0009-0005-5652-617X}{Gabriel Santos de Souza}
\newcommand{\orcid}[2]{
  \href{#1}{\includegraphics[scale=0.06]{img/orcid.pdf}\hspace{1mm}#2}
}
% Configuração ABNT.
\titulo{\uppercase{Inteligência Artificial na Universidade Federal de Sergipe: Como o Seu Uso pode Comprometer o Aprendizado em Disciplinas Introdutórias de Programação}}
\autor{
  Alexis Cristian Pertile de Oliveira Filho \and
  Bruno Danton Carneiro Silva \and
  Dávisson Cavalcante Costa \and
  Gabriel Ferreira Bernardo \and
  \orcid{https://orcid.org/0009-0005-5652-617X}{Gabriel Santos de Souza} \and
  Wevellyn Victória da Silva Azevedo
}
\orientador{Marcos Venícius Santos}
\instituicao{Universidade Federal de Sergipe -- UFS}
\local{São Cristóvão}
\data{2025}
\preambulo{Artigo científico apresentado à Universidade Federal de Sergipe como requisito de avaliação parcial da disciplina de Métodos e Técnicas de Pesquisa para Computação e, por conseguinte, a obtenção dos seus créditos.}
% Configuração do PDF.
\makeatletter
\hypersetup{
  pdftitle={\@title},
  pdfauthor={\@author},
  pdfsubject={\imprimirpreambulo},
  pdfcreator={LaTeX with abnTeX2},
  pdfkeywords={universidade federal de sergipe}{federal university of sergipe}{inteligencia artificial}{artificial intelligence}{educação}{education},
  colorlinks=true, % Mudar para "false" se for imprimir.
  linkcolor=blue,
  citecolor=blue,
  filecolor=magenta,
  urlcolor=blue,
  bookmarksdepth=4
}
\makeatother
% Espaçamento.
\setlrmarginsandblock{3cm}{3cm}{*}
\setulmarginsandblock{3cm}{3cm}{*}
\checkandfixthelayout
\setlength{\parindent}{1.3cm}
\setlength{\parskip}{0.2cm}
\SingleSpacing
% Documento.
\makeindex % Compila o índice.
\begin{document}

\selectlanguage{brazilian}
\frenchspacing % Espaçamento normal entre frases. O LaTeX usa o espaçamento arcaico de 2 espaços em vez de 1.

\imprimircapaelogo

\imprimirfolhaderosto

\begin{siglas}
\item[GPT] \textit{Generative Pretrained Transformer}
\item[IA] Inteligência Artificial
\item[LLM(s)] \textit{Large Language Model(s)}
\item[MIT] \textit{Massachusetts Institute of Technology}
\item[ML] \textit{Machine Learning}
\item[RNA(s)] Rede(s) Neural(ais) Artificial(ais)
\end{siglas}

\tableofcontents* % O "*" imprime o sumário sem indicar a página do próprio sumário.

\cleardoublepage

\textual

\section{Introdução}

Ferramentas digitais podem ser encontradas em diversos campos \parencite{cornelio_historia_2021}, tendo como ferramenta de destaque a Inteligência Artificial, devido à multifuncionalidade, podendo realizar uma ampla variedade de tarefas diferentes, e rapidez \cite{google_global_2025}. No meio acadêmico, o uso dessas ferramentas gera incertezas. A utilização inconsequente de LLMs traz riscos como a dependência criativa e decisória, limitando o desenvolvimento crítico dos universitários \cite{marrOs15Maiores2023}.

Esse uso já impacta negativamente a retenção de conhecimento \cite{leeImpactGenerativeAI2025}. Embora focado em profissionais, o resultado é relevante para a educação. Pesquisa do MIT \parencite{kosmynaYourBrainChatGPT2025} corroborou esse cenário: participantes que utilizaram IA tiveram desempenho inferior comparado aos que usaram mecanismos de busca ou apenas o cérebro. Mesmo ao alternarem para tarefas sem auxílio, o grupo que iniciou com IA manteve desempenho pior.

A popularização de LLMs como o ChatGPT no meio acadêmico torna o tema relevante. É necessário compreender o impacto do uso indiscriminado no amadurecimento cognitivo, incluindo a capacidade de aprendizado, tomada de decisões e pensamento crítico. Uma revisão sistemática de 70 estudos \parencite{delikoura2025superficialoutputssuperficiallearning} revelou que a interação frequente com LLMs pode provocar um efeito de \enquote{compreensão superficial} do conteúdo e dependência da ferramenta. A pesquisa visa investigar os efeitos do uso excessivo de geradores de texto em disciplinas de programação e suas consequências, como a redução da criatividade, raciocínio lógico, memória e atenção.

A importância da pesquisa abrange o contexto educacional e o mercado de trabalho, visto que estudantes dependentes de tecnologia serão os futuros profissionais. A análise questiona como tratar a dependência de IAs e garantir a formação crítica em uma realidade automatizada. A solução reside na administração do uso consciente e equilibrado da tecnologia, não em proibição, visando resultados educacionais significativos.2

O objetivo geral deste artigo é de buscar uma forma de integrar a IA na educação dessas disciplinas, em conjunto com os professores, sem afetar o aprendizado dos alunos.

Os objetivos específicos incluem:

\begin{enumerate}
\item Identificar a frequência do uso de inteligência artificial pelos alunos.
\item Mapear os efeitos percebidos por professores na capacidade cognitiva.
\item Observar os contextos de uso intenso da tecnologia em estudo. % Especificar.
\end{enumerate}

\section{Referencial Teórico}

Como LLMs são uma nova e popular tecnologia \parencite{openai_funding_2025}, torna-se necessário medir o impacto que o seu uso pode causar. Seja desde ao consumo de eletricidade \parencite{10748289}, até, como foco deste artigo, o aprendizado \cite{10983758}. Neste trabalho, será avaliado o impacto destas ferramentas para a aprendizagem de programação, mediante uma pesquisa com os discentes ingressantes do período 2025.1. Isso auxiliará, por exemplo, no momento de decisão dos docentes, quando forem organizar o que irão fazer a respeito do uso destas tecnologias para as novas matérias da reformulação da grade, \textit{viz.} \enquote{Programação A}, \enquote{Programação B} e \enquote{Programação C}.

\textit{Large Language Models}, exemplificados pelo ChatGPT e Gemini, constituem modelos de Inteligência Artificial Generativa treinados em extensos conjuntos de dados textuais \cite{raffelExploringLimitsTransfer2019}. Tais sistemas viabilizam a geração de textos, a produção de códigos e a sumarização de informações \cite{google_global_2025}. Observa-se a difusão dessas tecnologias no meio acadêmico a partir de 2023, motivada pela acessibilidade e pela aplicabilidade prática das soluções oferecidas \cite{jensen_generative_2025}. Verifica-se que a disponibilidade de respostas imediatas, especificamente em tarefas de programação, altera a dinâmica do aprendizado discente. Discute-se, consequentemente, a atualização curricular visando a preservação e o desenvolvimento do raciocínio lógico \cite{gan_large_2023}.

\subsection{Inteligência Artificial}

A Inteligência Artificial precede a própria consolidação da computação moderna. Em seu trabalho seminal de 1950, Alan Turing propôs a questão: \enquote{podem as máquinas pensar?}, estabelecendo o \enquote{Jogo da Imitação} (posteriormente conhecido como \enquote{Teste de Turing}) como um critério operacional para a inteligência \cite{turing_computing_1950}. No entanto, o termo \enquote{Inteligência Artificial} foi formalmente cunhado apenas em 1956, durante a conferência de Dartmouth, onde John McCarthy e outros pesquisadores propuseram que \textquote{cada aspecto do aprendizado ou qualquer outra característica da inteligência pode, em princípio, ser descrita com tamanha precisão que uma máquina pode ser feita para simulá-la} \cite{mccarthy_proposal_2006}. Desde então, a área evoluiu de sistemas baseados em regras simbólicas para abordagens estatísticas robustas.

A IA é definida não apenas pela simulação do pensamento humano, mas pela construção de agentes inteligentes. Segundo \textcite{russell_artificial_2021}, um agente inteligente é uma entidade que percebe seu ambiente por meio de sensores e age sobre esse ambiente através de atuadores, visando maximizar as chances de sucesso em uma determinada tarefa. Essa definição abrange desde algoritmos simples de controle até sistemas complexos de processamento de linguagem natural. A onipresença dessas tecnologias é evidenciada pelo crescimento do otimismo global e da utilização de ferramentas de IA em diversos setores produtivos e sociais \cite{google_global_2025}.

\subsection{Aprendizado de Máquina}

Dentro do vasto campo da IA, destaca-se o Aprendizado de Máquina (\textit{Machine Learning} - ML) como o subcampo responsável pelos avanços mais significativos das últimas décadas. Diferentemente da programação tradicional, onde as regras são explicitamente codificadas, o Aprendizado de Máquina foca no desenvolvimento de algoritmos que melhoram seu desempenho automaticamente através da experiência. A definição formal amplamente aceita é dada por \textcite{mitchell_machine_1997}: diz-se que um programa de computador aprende a partir da experiência \(E\), em relação a uma classe de tarefas \(T\) e medida de desempenho \(P\), se seu desempenho em \(T\), medido por \(P\), melhora com a experiência \(E\). Essa mudança de paradigma permitiu a resolução de problemas complexos, como reconhecimento de padrões e previsões estatísticas, que eram intratáveis via programação convencional.

A aplicação dessas tecnologias no cenário educacional introduz novos paradigmas e desafios. A introdução de modelos generativos, uma vertente avançada do ML, exige uma reavaliação das práticas pedagógicas. A \textcite{unesco_guidance_2023} destaca que, embora a IA generativa ofereça oportunidades para personalizar o aprendizado, a implementação requer diretrizes éticas rigorosas para garantir que a tecnologia aumente as capacidades humanas em vez de substituí-las ou atrofiar o pensamento crítico. Observa-se, portanto, uma tensão entre a eficiência oferecida pelos sistemas automatizados e a necessidade de preservar a integridade cognitiva dos estudantes \cite{marrOs15Maiores2023, gan_large_2023}.

Compreende-se, assim, que a Inteligência Artificial e o Aprendizado de Máquina constituem a base sobre a qual as tecnologias modernas de processamento de informação são construídas. Para entender o funcionamento dos Grandes Modelos de Linguagem (LLMs), objeto central deste estudo, faz-se necessário aprofundar-se nas estruturas que possibilitam esse aprendizado complexo. O próximo tópico abordará as Redes Neurais Artificiais e o \textit{Deep Learning}, mecanismos que mimetizam a estrutura neural biológica para processar informações em camadas hierárquicas de abstração.

\subsection{Redes Neurais Artificiais}

A evolução do Aprendizado de Máquina para lidar com dados de alta complexidade e não-linearidade fundamenta-se no desenvolvimento das Redes Neurais Artificiais (RNAs). Inspiradas biologicamente no funcionamento do sistema nervoso central, as RNAs tiveram gênese teórica estabelecida por \textcite{mcculloch_logical_1943}, que propuseram o primeiro modelo matemático de um neurônio artificial. Nesse modelo, a unidade de processamento recebe múltiplos sinais de entrada, pondera-os através de pesos sinápticos e aplica uma função de ativação para determinar a saída. Embora os modelos iniciais fossem limitados em capacidade de representação, o desenvolvimento do algoritmo de \textit{backpropagation} (retropropagação) permitiu o ajuste eficiente desses pesos em redes com múltiplas camadas, viabilizando o aprendizado supervisionado complexo \cite{rumelhartLearningRepresentationsBackpropagating1986}.

\subsection{Deep Learning}

O conceito de \textit{Deep Learning} (Aprendizado Profundo) emerge como uma subcategoria das RNAs, caracterizada pela utilização de múltiplas camadas ocultas de processamento entre a entrada e a saída. Segundo \textcite{goodfellow_deep_2016}, a profundidade dessas arquiteturas permite que o computador construa conceitos complexos a partir de conceitos mais simples. Em uma abordagem de \textit{Deep Learning}, as camadas iniciais podem identificar características básicas, como arestas em uma imagem, enquanto as camadas subsequentes combinam essas características para identificar formas, objetos e, finalmente, cenas completas. Essa hierarquia de representações elimina a necessidade de extração manual de características (\textit{feature engineering}), que era um gargalo nos métodos tradicionais de IA.

A eficácia do \textit{Deep Learning} reside na capacidade de generalização em grandes volumes de dados. \textcite{lecun_deep_2015} explicam que essas técnicas aprimoraram drasticamente o estado da arte em reconhecimento de fala, reconhecimento visual e processamento de linguagem natural. O treinamento dessas redes envolve a minimização de uma função de custo, ajustando iterativamente os parâmetros internos da rede para reduzir a discrepância entre a previsão do modelo e o resultado real. A arquitetura dessas redes varia conforme a aplicação, incluindo Redes Neurais Convolucionais (CNNs) para processamento de imagens e Redes Neurais Recorrentes (RNNs) para dados sequenciais, sendo estas últimas precursoras diretas da arquitetura Transformer.

No entanto, a implementação de modelos de \textit{Deep Learning} impõe custos computacionais e energéticos significativos. A infraestrutura necessária para treinar modelos profundos, especialmente aqueles com bilhões de parâmetros como os LLMs atuais, demanda \textit{data centers} de grande escala. Estudos recentes, como o de \textcite{10748289}, analisam o impacto crescente da Inteligência Artificial no consumo global de eletricidade. Verifica-se que a complexidade computacional não apenas eleva os custos operacionais, mas também levanta questões sobre a sustentabilidade ambiental do avanço tecnológico contínuo nessa área.

Conclui-se que o \textit{Deep Learning} representa a base tecnológica que viabilizou o surgimento da IA Generativa moderna. A capacidade de processar e gerar informações complexas, contudo, exigiu uma nova evolução arquitetural para lidar especificamente com as nuances e dependências de longa distância da linguagem humana. Esse avanço materializou-se na arquitetura Transformer, que superou as limitações das redes recorrentes anteriores e será detalhada na próxima seção (\textcites{vaswani_attention_2017}).

\subsection{Arquitetura Transformer}

A consolidação dos Grandes Modelos de Linguagem atuais fundamenta-se na introdução da arquitetura Transformer, proposta originalmente por \textcite{vaswani_attention_2017}. Até o surgimento dessa arquitetura, o processamento de sequências (como textos e áudios) dependia majoritariamente de Redes Neurais Recorrentes (RNNs) e unidades LSTM (\textit{Long Short-Term Memory}). Tais modelos processavam os dados de forma sequencial, palavra por palavra, o que impedia a paralelização eficiente do treinamento e limitava a capacidade do sistema de \enquote{lembrar} contextos distantes dentro de um parágrafo longo. O modelo Transformer rompeu com esse paradigma ao dispensar a recorrência em favor de mecanismos de atenção pura, permitindo o processamento simultâneo de toda a sequência de dados.

O núcleo funcional dessa arquitetura reside no mecanismo de \enquote{Autoatenção} (\textit{Self-Attention}). Conforme detalhado por \textcite{vaswani_attention_2017}, esse mecanismo permite que o modelo, ao processar uma palavra específica, analise e pondere a relevância de todas as outras palavras da frase instantaneamente, independentemente da distância física entre elas no texto. Matematicamente, isso é realizado através de operações de álgebra linear que calculam pontuações de compatibilidade entre vetores de consulta (\textit{query}), chave (\textit{key}) e valor (\textit{value}). Dessa forma, o modelo consegue desambiguar termos polissêmicos baseando-se no contexto global da sentença, e não apenas nas palavras imediatamente vizinhas.

Outro componente crítico introduzido para compensar a ausência de processamento sequencial é a Codificação Posicional (\textit{Positional Encoding}). Segundo \textcite{jurafsky_speech_2024}, como o Transformer \enquote{enxerga} a frase inteira de uma vez, ele não possui uma noção inerente de ordem (saber qual palavra vem antes ou depois). Para solucionar isso, injeta-se informações sobre a posição relativa ou absoluta dos tokens diretamente nas representações de entrada, permitindo que a rede distinga a ordem sintática dos elementos. Essa estrutura é composta por blocos empilhados de codificadores e decodificadores, embora muitos modelos modernos (como a série GPT) utilizem apenas a pilha de decodificadores.

A principal vantagem prática dessa arquitetura é a escalabilidade. A eliminação da dependência sequencial permitiu que pesquisadores treinassem modelos em \textit{datasets} massivos utilizando milhares de GPUs em paralelo, algo inviável com as tecnologias anteriores. Isso resultou em uma capacidade de generalização sem precedentes, onde o modelo pré-treinado em um vasto corpus textual pode ser ajustado (\textit{fine-tuned}) para tarefas específicas com alto desempenho.

Entende-se, portanto, que o Transformer não é apenas uma melhoria incremental, mas a infraestrutura base que viabilizou a existência dos \textit{Large Language Models} (LLMs). É a capacidade dessa arquitetura de manipular contextos longos e aprender representações semânticas profundas que permite aos sistemas atuais, como o ChatGPT, gerarem textos coerentes e contextualmente ricos. O próximo tópico detalhará especificamente a classe dos LLMs, explorando como essa arquitetura é treinada em escala global.

\subsection{Large Language Models (LLMs)}

Os \textit{Large Language Models} (LLMs) representam a aplicação da arquitetura Transformer em uma escala massiva, tanto em termos de parâmetros computacionais quanto de volume de dados de treinamento. De acordo com \textcite{zhao_survey_2023}, o termo \foreignquote{english}{Large} refere-se a modelos que contêm de dezenas a centenas de bilhões de parâmetros. Diferentemente de modelos menores, os LLMs exibem \enquote{habilidades emergentes}, ou seja, capacidades de raciocínio e resolução de problemas que não foram explicitamente programadas, mas que surgem espontaneamente à medida que a complexidade do modelo ultrapassa certos limiares computacionais.

O processo de construção desses modelos ocorre fundamentalmente em duas etapas: o pré-treinamento e o ajuste fino (\textit{fine-tuning}). Na fase de pré-treinamento, o modelo é exposto a vastos corpora textuais (livros, artigos, código-fonte e páginas da web) com o objetivo estatístico de prever o próximo token em uma sequência. \textcite{brown_language_2020} demonstraram que, ao aumentar drasticamente a escala desse treinamento, o modelo adquire a capacidade de \textit{few-shot learning} (aprendizado com poucos exemplos), conseguindo realizar tarefas novas apenas observando algumas instruções no contexto, sem a necessidade de retreinamento dos seus pesos.

No entanto, a simples previsão de texto não garante que o modelo seja útil ou seguro para interação humana. Para mitigar a geração de conteúdo tóxico ou alucinatório, aplica-se a técnica de Aprendizado por Reforço com Feedback Humano (RLHF). Conforme detalhado por \textcite{ouyang_training_2022}, essa metodologia consiste em coletar avaliações humanas sobre as respostas geradas pelo modelo e treinar uma \enquote{função de recompensa} que guia o LLM a alinhar suas saídas com as intenções e valores do usuário. Foi a implementação bem-sucedida dessa técnica que permitiu a transição de modelos puramente probabilísticos (como o GPT-3 original) para assistentes conversacionais instruídos (como o ChatGPT e o Gemini).

A versatilidade dos LLMs estende-se para além da geração de linguagem natural. Observa-se eficácia crescente em tarefas de raciocínio lógico, escrita de código de programação e análise de sentimentos. \textcite{gan_large_2023} ressaltam que essa multifuncionalidade posiciona os LLMs como ferramentas de propósito geral, capazes de atuar como tutores inteligentes ou assistentes de codificação. Contudo, essa mesma capacidade levanta preocupações sobre a confiabilidade das informações, visto que o modelo opera baseado em correlações estatísticas e não em uma compreensão factual da realidade, estando sujeito a \enquote{alucinações} (geração de informações falsas com alta confiança).

Em suma, os LLMs constituem o estado da arte na interseção entre processamento de linguagem e inteligência artificial generativa. A disponibilidade dessas ferramentas no ambiente acadêmico altera a relação entre o estudante e o conhecimento, uma vez que a barreira para a produção de textos complexos e códigos funcionais é drasticamente reduzida. O impacto específico dessa tecnologia na educação e no desenvolvimento do raciocínio lógico, foco central desta pesquisa, será discutido na seção seguinte.

\subsection{Impactos na Educação e no Desenvolvimento do Raciocínio Lógico}

A integração de Grandes Modelos de Linguagem (LLMs) no ambiente educacional precipitou uma transformação nas dinâmicas de ensino e aprendizagem, particularmente em disciplinas que exigem raciocínio lógico estruturado, como a programação. Conforme análise de \textcite{jensen_generative_2025}, a academia oscila entre a percepção da IA como uma ferramenta de personalização do ensino e o receio de que ela promova a atrofia de competências fundamentais. Embora a eficiência na produção de respostas seja inegável, investiga-se se a facilidade de obtenção de soluções prontas compromete a retenção do conhecimento e o desenvolvimento de habilidades cognitivas profundas.

Estudos recentes baseados em neurociência fornecem evidências quantitativas sobre os efeitos dessa dependência tecnológica. Em uma investigação utilizando eletroencefalografia (EEG), \textcite{kosmynaYourBrainChatGPT2025} demonstraram que estudantes que utilizam LLMs para realizar tarefas acadêmicas exibem uma conectividade cerebral significativamente menor nas redes neurais associadas à memória e ao planejamento, em comparação com aqueles que realizam a tarefa sem auxílio ou apenas com motores de busca. O estudo introduz o conceito de \enquote{dívida cognitiva}, sugerindo que o uso contínuo dessas ferramentas sem o devido esforço mental resulta em um desempenho neural e comportamental inferior a longo prazo, mesmo quando o estudante volta a realizar tarefas sem a ferramenta.

No contexto específico do pensamento crítico, observa-se uma correlação inversa entre a confiança na IA e o engajamento cognitivo do aluno. \textcite{leeImpactGenerativeAI2025} relatam, através de uma pesquisa com trabalhadores do conhecimento, que altos níveis de confiança na precisão do modelo levam a uma redução na verificação dos resultados e no exercício do ceticismo. Esse fenômeno é particularmente crítico no ensino de programação, onde o processo de \enquote{depuração} (\textit{debugging}) e a compreensão da sintaxe são essenciais para a formação da lógica algorítmica. Ao delegar a escrita do código à IA, o estudante pode perder a oportunidade de vivenciar o erro construtivo, etapa fundamental para a consolidação do aprendizado.

Adicionalmente, \textcite{delikoura2025superficialoutputssuperficiallearning} alertam para o risco do \enquote{aprendizado superficial} decorrente de \enquote{saídas superficiais}. A facilidade com que os LLMs geram textos fluentes e códigos funcionais pode criar uma ilusão de competência, onde o estudante acredita dominar o conteúdo apenas por ser capaz de gerar o produto final, sem compreender os processos subjacentes. Esse cenário impõe desafios urgentes às instituições de ensino, que precisam reformular métodos de avaliação para focar menos no produto final e mais no processo de raciocínio e na defesa oral das soluções apresentadas.

Em suma, a literatura atual indica que, embora os LLMs possuam potencial como tutores auxiliares, seu uso irrestrito e não supervisionado tende a substituir, em vez de complementar, o esforço cognitivo necessário para o aprendizado profundo. Diante da onipresença dessas ferramentas, conclui-se que a adaptação curricular não é apenas desejável, mas imperativa para garantir que as futuras gerações de profissionais mantenham a capacidade de pensar, analisar e criar de forma autônoma.

\postextual

\cleardoublepage

\printbibliography

\end{document}
