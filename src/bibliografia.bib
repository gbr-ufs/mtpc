@online{cornelio_historia_2021,
  title =        {História da tecnologia: da pré-história ao
                  {Metaverso} • {Usemobile}},
  shorttitle =   {História da tecnologia},
  url =          {https://usemobile.com.br/historia-da-tecnologia/},
  abstract =     {Confira a história da tecnologia, conhecendo desde
                  os primórdios da humanidade até os adventos
                  contemporâneos.},
  language =     {pt-BR},
  urldate =      {2025-11-20},
  journal =      {Usemobile},
  author =       {Cornélio, Angelina},
  month =        nov,
  year =         {2021}
}

@online{google_global_2025,
  title =        {Global {AI} {Optimism} {Increases} as {Usage}
                  {Grows}},
  url =
                  {https://publicpolicy.google/article/global-ai-optimism-increases-as-usage-grows/},
  abstract =     {Based on the second annual global survey from Ipsos
                  and Google surveying 21,000 citizens across 21
                  countries, attitudes towards AI are trending more
                  positive as its use becomes more widespread.},
  language =     {en},
  urldate =      {2025-11-20},
  journal =      {Google Public Policy},
  author =       {Marr, Bernard},
  year =         {2025}
}

@online{marrOs15Maiores2023,
  type =         {News},
  title =        {Os 15 maiores riscos da inteligência artificial},
  author =       {Marr, Bernard},
  date =         {2023-06-05T06:00:55-03:00},
  url =
                  {https://forbes.com.br/forbes-tech/2023/06/os-15-maiores-riscos-da-inteligencia-artificial/},
  urldate =      {2025-11-16},
  abstract =     {Today we’re announcing new funding—\$40B at a \$300B
                  post-money valuation, which enables us to push the
                  frontiers of AI research even further, scale our
                  compute infrastructure, and deliver increasingly
                  powerful tools for the 500 million people who use
                  ChatGPT every week.},
  langid =       {brazilian},
  organization = {Forbes Brasil},
  year =         {2023}
}

@inproceedings{leeImpactGenerativeAI2025,
  title =        {The {{Impact}} of {{Generative AI}} on {{Critical
                  Thinking}}: {{Self-Reported Reductions}} in
                  {{Cognitive Effort}} and {{Confidence Effects From}}
                  a {{Survey}} of {{Knowledge Workers}}},
  shorttitle =   {The {{Impact}} of {{Generative AI}} on {{Critical
                  Thinking}}},
  booktitle =    {Proceedings of the 2025 {{CHI Conference}} on
                  {{Human Factors}} in {{Computing Systems}}},
  author =       {Lee, Hao-Ping (Hank) and Sarkar, Advait and
                  Tankelevitch, Lev and Drosos, Ian and Rintel, Sean
                  and Banks, Richard and Wilson, Nicholas},
  date =         {2025-04-25},
  series =       {{{CHI}} '25},
  pages =        {1--22},
  publisher =    {Association for Computing Machinery},
  location =     {New York, NY, USA},
  doi =          {10.1145/3706598.3713778},
  url =          {https://dl.acm.org/doi/10.1145/3706598.3713778},
  urldate =      {2025-11-16},
  abstract =     {The rise of Generative AI (GenAI) in knowledge
                  workflows raises questions about its impact on
                  critical thinking skills and practices. We survey
                  319 knowledge workers to investigate 1) when and how
                  they perceive the enaction of critical thinking when
                  using GenAI, and 2) when and why GenAI affects their
                  effort to do so. Participants shared 936 first-hand
                  examples of using GenAI in work tasks.
                  Quantitatively, when considering both task- and
                  user-specific factors, a user’s task-specific
                  self-confidence and confidence in GenAI are
                  predictive of whether critical thinking is enacted
                  and the effort of doing so in GenAI-assisted tasks.
                  Specifically, higher confidence in GenAI is
                  associated with less critical thinking, while higher
                  self-confidence is associated with more critical
                  thinking. Qualitatively, GenAI shifts the nature of
                  critical thinking toward information verification,
                  response integration, and task stewardship. Our
                  insights reveal new design challenges and
                  opportunities for developing GenAI tools for
                  knowledge work.},
  isbn =         {979-8-4007-1394-1},
  year =         2025
}

@online{kosmynaYourBrainChatGPT2025,
  title =        {Your {{Brain}} on {{ChatGPT}}: {{Accumulation}} of
                  {{Cognitive Debt}} When {{Using}} an {{AI
                  Assistant}} for {{Essay Writing Task}}},
  shorttitle =   {Your {{Brain}} on {{ChatGPT}}},
  author =       {Kosmyna, Nataliya and Hauptmann, Eugene and Yuan, Ye
                  Tong and Situ, Jessica and Liao, Xian-Hao and
                  Beresnitzky, Ashly Vivian and Braunstein, Iris and
                  Maes, Pattie},
  date =         {2025-06-10},
  eprint =       {2506.08872},
  eprinttype =   {arXiv},
  eprintclass =  {cs},
  doi =          {10.48550/arXiv.2506.08872},
  url =          {http://arxiv.org/abs/2506.08872},
  urldate =      {2025-11-16},
  abstract =     {This study explores the neural and behavioral
                  consequences of LLM-assisted essay writing.
                  Participants were divided into three groups: LLM,
                  Search Engine, and Brain-only (no tools). Each
                  completed three sessions under the same condition.
                  In a fourth session, LLM users were reassigned to
                  Brain-only group (LLM-to-Brain), and Brain-only
                  users were reassigned to LLM condition
                  (Brain-to-LLM). A total of 54 participants took part
                  in Sessions 1-3, with 18 completing session 4. We
                  used electroencephalography (EEG) to assess
                  cognitive load during essay writing, and analyzed
                  essays using NLP, as well as scoring essays with the
                  help from human teachers and an AI judge. Across
                  groups, NERs, n-gram patterns, and topic ontology
                  showed within-group homogeneity. EEG revealed
                  significant differences in brain connectivity:
                  Brain-only participants exhibited the strongest,
                  most distributed networks; Search Engine users
                  showed moderate engagement; and LLM users displayed
                  the weakest connectivity. Cognitive activity scaled
                  down in relation to external tool use. In session 4,
                  LLM-to-Brain participants showed reduced alpha and
                  beta connectivity, indicating under-engagement.
                  Brain-to-LLM users exhibited higher memory recall
                  and activation of occipito-parietal and prefrontal
                  areas, similar to Search Engine users. Self-reported
                  ownership of essays was the lowest in the LLM group
                  and the highest in the Brain-only group. LLM users
                  also struggled to accurately quote their own work.
                  While LLMs offer immediate convenience, our findings
                  highlight potential cognitive costs. Over four
                  months, LLM users consistently underperformed at
                  neural, linguistic, and behavioral levels. These
                  results raise concerns about the long-term
                  educational implications of LLM reliance and
                  underscore the need for deeper inquiry into AI's
                  role in learning.},
  pubstate =     {prepublished},
  keywords =     {Computer Science - Artificial Intelligence},
  year =         {2025}
}

@misc{delikoura2025superficialoutputssuperficiallearning,
  title =        {From Superficial Outputs to Superficial Learning:
                  Risks of Large Language Models in Education},
  author =       {Iris Delikoura and Yi. R Fung and Pan Hui},
  year =         2025,
  eprint =       {2509.21972},
  archivePrefix ={arXiv},
  primaryClass = {cs.CY},
  url =          {https://arxiv.org/abs/2509.21972}
}

@online{openai_funding_2025,
  title =        {New funding to build towards {AGI}},
  url =          {https://openai.com/index/march-funding-updates/},
  abstract =     {Today we’re announcing new funding—\$40B at a \$300B
                  post-money valuation, which enables us to push the
                  frontiers of AI research even further, scale our
                  compute infrastructure, and deliver increasingly
                  powerful tools for the 500 million people who use
                  ChatGPT every week.},
  author =       {OpenAI},
  language =     {en-US},
  urldate =      {2025-11-21},
  year =         {2025}
}

@INPROCEEDINGS{10748289,
  author =       {Jia, Yuelong},
  booktitle =    {2024 3rd International Conference on Artificial
                  Intelligence, Internet of Things and Cloud Computing
                  Technology (AIoTC)},
  title =        {Analysis of the Impact of Artificial Intelligence on
                  Electricity Consumption},
  year =         2024,
  pages =        {57-60},
  keywords =     {Data centers;Electricity;Computational
                  modeling;Energy conservation;Demand response;Power
                  markets;Energy efficiency;Artificial
                  intelligence;Carbon;Uninterruptible power
                  systems;artificial intelligence;electricity
                  demand;demand response;energy efficiency},
  doi =          {10.1109/AIoTC63215.2024.10748289}
}

@INPROCEEDINGS{10983758,
  author =       {Al Azri, Kawther Zahir and Tumati, Raja and Al
                  Tarshi, Shahd Saud},
  booktitle =    {2025 International Conference for Artificial
                  Intelligence, Applications, Innovation and Ethics
                  (AI2E)},
  title =        {Impact of Artificial Intelligence on Student
                  Learning in Oman},
  year =         2025,
  pages =        {1-5},
  keywords =     {Surveys;Technological
                  innovation;Ethics;Education;Learning (artificial
                  intelligence);Chatbots;Internet;Artificial
                  intelligence tools;student learning;AI benefits;AI
                  challenges;AI education},
  doi =          {10.1109/AI2E64943.2025.10983758}
}

@article{jensen_generative_2025,
  title =        {Generative {AI} and higher education: a review of
                  claims from the first months of {ChatGPT}},
  volume =       89,
  issn =         {1573-174X},
  shorttitle =   {Generative {AI} and higher education},
  url =          {https://doi.org/10.1007/s10734-024-01265-3},
  doi =          {10.1007/s10734-024-01265-3},
  abstract =     {The release of the Artificial Intelligence (AI)
                  chatbot ChatGPT renewed discussions about how AI
                  would upend higher education. This paper presents a
                  critical analysis of “grey literature” claims made
                  in the first months after ChatGPT was made public,
                  exploring what these discussions might mobilise in
                  practice. We identified articles for inclusion
                  through a systematic search of five prominent higher
                  education sector outlets. The included articles were
                  thematically coded for claims about generative AI
                  and higher education. We identified ten claims:
                  Three about the nature of ChatGPT, four about
                  changing practices of institutions and teachers, and
                  three about new independent practices of students.
                  Overall, the claims present a positive perspective
                  on AI in higher education. While being perceived as
                  a disruption of the status quo, the authors
                  generally frame AI as a catalyst for existing
                  agendas, e.g. assessment reform, personalisation, or
                  inclusion. This suggests a focus on embracing the
                  affordances offered by AI and primarily addressing
                  risks by including AI in curricula. Furthermore, the
                  claims mainly portray students as either plagiarists
                  or victims of a failing educational system. The
                  paper proposes that a more critical interrogation of
                  generative AI, and the involvement of students in
                  the conversation, may be beneficial.},
  language =     {en},
  number =       4,
  urldate =      {2025-11-21},
  journal =      {Higher Education},
  author =       {Jensen, Lasse X. and Buhl, Alexandra and Sharma,
                  Anjali and Bearman, Margaret},
  month =        apr,
  year =         2025,
  keywords =     {ChatGPT, Artificial Intelligence, Generative AI,
                  Higher education, Grey literature, Scoping review},
  pages =        {1145--1161}
}

@inproceedings{gan_large_2023,
  title =        {Large {Language} {Models} in {Education}: {Vision}
                  and {Opportunities}},
  shorttitle =   {Large {Language} {Models} in {Education}},
  url =
                  {https://ieeexplore.ieee.org/abstract/document/10386291},
  doi =          {10.1109/BigData59044.2023.10386291},
  abstract =     {With the rapid development of artificial
                  intelligence technology, large language models
                  (LLMs) have become a hot research topic. Education
                  plays an important role in human social development
                  and progress. Traditional education faces challenges
                  such as individual student differences, insufficient
                  allocation of teaching resources, and assessment of
                  teaching effectiveness. Therefore, the applications
                  of LLMs in the field of digital/smart education have
                  broad prospects. The research on educational large
                  models (EduLLMs) is constantly evolving, providing
                  new methods and approaches to achieve personalized
                  learning, intelligent tutoring, and educational
                  assessment goals, thereby improving the quality of
                  education and the learning experience. This article
                  aims to investigate and summarize the application of
                  LLMs in smart education. It first introduces the
                  research background and motivation of LLMs and
                  explains the essence of LLMs. It then discusses the
                  relationship between digital education and EduLLMs
                  and summarizes the current research status of
                  educational large models. The main contributions are
                  the systematic summary and vision of the research
                  background, motivation, and application of large
                  models for education (LLM4Edu). By reviewing
                  existing research, this article provides guidance
                  and insights for educators, researchers, and
                  policy-makers to gain a deep understanding of the
                  potential and challenges of LLM4Edu. It further
                  provides guidance for further advancing the
                  development and application of LLM4Edu, while still
                  facing technical, ethical, and practical challenges
                  requiring further research and exploration.},
  urldate =      {2025-11-21},
  booktitle =    {2023 {IEEE} {International} {Conference} on {Big}
                  {Data} ({BigData})},
  author =       {Gan, Wensheng and Qi, Zhenlian and Wu, Jiayang and
                  Lin, Jerry Chun-Wei},
  month =        dec,
  year =         2023,
  keywords =     {Ethics, Analytical models, Systematics, Machine
                  vision, Education, Educational technology, Big Data,
                  artificial intelligence, LLMs, smart education,
                  vision, opportunities},
  pages =        {4776--4785},
}

@book{russell_artificial_2021,
  title =        {Artificial Intelligence: A Modern Approach},
  edition =      {4th},
  publisher =    {Pearson},
  author =       {Russell, Stuart J. and Norvig, Peter},
  year =         2021,
  address =      {Hoboken}
}

@article{mccarthyProposalDartmouthSummer1956,
  title =        {A {{Proposal}} for the {{Dartmouth Summer Research
                  Project}} on {{Artificial Intelligence}}},
  author =       {McCarthy, John and Minsky, Marvin and Rochester,
                  Nathaniel and Shannon, Claude E.},
  date =         {1956-08-31},
  url =
                  {https://ebiquity.umbc.edu/paper/html/id/1199/A-Proposal-for-the-Dartmouth-Summer-Research-Project-on-Artificial-Intelligence},
  urldate =      {2026-01-11},
  abstract =     {We propose that a 2 month, 10 man study of
                  artificial intelligence be carried out during the
                  summer of 1956 at Dartmouth College in Hanover, New
                  Hampshire. The study is to proceed on the basis of
                  the conjecture that every aspect of learning or any
                  other feature of intelligence can in principle be so
                  precisely described that a machine can be made to
                  simulate it. An attempt will be made to find how to
                  make machines use language, form abstractions and
                  concepts, solve kinds of problems now reserved for
                  humans, and improve themselves. We think that a
                  significant advance can be made in one or more of
                  these problems if a carefully selected group of
                  scientists work on it together for a summer.},
  langid =       {english},
  language =     {en}
}

@article{turing_computing_1950,
  title =        {Computing Machinery and Intelligence},
  volume =       59,
  number =       236,
  journal =      {Mind},
  author =       {Turing, A. M.},
  year =         1950,
  pages =        {433--460}
}

@book{mitchell_machine_1997,
  title =        {Machine Learning},
  publisher =    {McGraw-Hill},
  author =       {Mitchell, Tom M.},
  year =         1997,
  address =      {New York}
}

@online{unesco_guidance_2023,
  title =        {Guidance for generative AI in education and
                  research},
  url =          {https://unesdoc.unesco.org/ark:/48223/pf0000386693},
  author =       {{UNESCO}},
  year =         {2023},
  urldate =      {2025-11-21},
  organization = {United Nations Educational, Scientific and Cultural
                  Organization}
}

@book{goodfellow_deep_2016,
  title =        {Deep Learning},
  author =       {Goodfellow, Ian and Bengio, Yoshua and Courville,
                  Aaron},
  publisher =    {MIT Press},
  note =         {http://www.deeplearningbook.org},
  year =         2016,
  address =      {Cambridge, MA}
}

@article{lecun_deep_2015,
  title =        {Deep learning},
  volume =       521,
  number =       7553,
  journal =      {Nature},
  author =       {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year =         2015,
  pages =        {436--444},
  publisher =    {Nature Publishing Group}
}

@article{mcculloch_logical_1943,
  title =        {A logical calculus of the ideas immanent in nervous
                  activity},
  volume =       5,
  number =       4,
  journal =      {Bulletin of mathematical biophysics},
  author =       {McCulloch, Warren S. and Pitts, Walter},
  year =         1943,
  pages =        {115--133}
}

@inproceedings{vaswani_attention_2017,
  title =        {Attention is All You Need},
  booktitle =    {Advances in Neural Information Processing Systems},
  author =       {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki
                  and Uszkoreit, Jakob and Jones, Llion and Gomez,
                  Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  volume =       30,
  year =         2017,
  publisher =    {Curran Associates, Inc.},
  url =
                  {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}
}

@book{jurafsky_speech_2024,
  title =        {Speech and Language Processing: An Introduction to
                  Natural Language Processing, Computational
                  Linguistics, and Speech Recognition},
  author =       {Jurafsky, Daniel and Martin, James H.},
  edition =      {3rd draft},
  year =         2024,
  publisher =    {Online draft},
  url =          {https://web.stanford.edu/~jurafsky/slp3/}
}

@article{brown_language_2020,
  title =        {Language Models are Few-Shot Learners},
  author =       {Brown, Tom and Mann, Benjamin and Ryder, Nick and
                  Subbiah, Melanie and Kaplan, Jared and Dhariwal,
                  Prafulla and Neelakantan, Arvind and Shyam, Pranav
                  and Sastry, Girish and Askell, Amanda},
  journal =      {Advances in Neural Information Processing Systems},
  volume =       33,
  pages =        {1877--1901},
  year =         2020,
  url =
                  {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}
}

@article{ouyang_training_2022,
  title =        {Training language models to follow instructions with
                  human feedback},
  author =       {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and
                  Almeida, Diogo and Wainwright, Carroll and Mishkin,
                  Pamela and Zhang, Chong and Agarwal, Sandhini and
                  Slama, Katarina and Ray, Alex},
  journal =      {Advances in Neural Information Processing Systems},
  volume =       35,
  pages =        {27730--27744},
  year =         2022,
  url =
                  {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a0017cc-Paper.pdf}
}

@article{zhao_survey_2023,
  title =        {A Survey of Large Language Models},
  author =       {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and
                  Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and
                  Min, Yingqian and Zhang, Beichen and Zhang, Junjie
                  and Dong, Zican},
  journal =      {arXiv preprint arXiv:2303.18223},
  year =         2023,
  url =          {https://arxiv.org/abs/2303.18223}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title =        {Learning Representations by Back-Propagating Errors},
  author =       {Rumelhart, David E. and Hinton, Geoffrey E. and
                  Williams, Ronald J.},
  date =         {1986-10},
  journaltitle = {Nature},
  volume =       {323},
  number =       {6088},
  pages =        {533--536},
  publisher =    {Nature Publishing Group},
  issn =         {1476-4687},
  doi =          {10.1038/323533a0},
  url =          {https://www.nature.com/articles/323533a0},
  urldate =      {2025-12-04},
  abstract =     {We describe a new learning procedure,
                  back-propagation, for networks of neurone-like
                  units. The procedure repeatedly adjusts the weights
                  of the connections in the network so as to minimize
                  a measure of the difference between the actual
                  output vector of the net and the desired output
                  vector. As a result of the weight adjustments,
                  internal ‘hidden’ units which are not part of the
                  input or output come to represent important features
                  of the task domain, and the regularities in the task
                  are captured by the interactions of these units. The
                  ability to create useful new features distinguishes
                  back-propagation from earlier, simpler methods such
                  as the perceptron-convergence procedure1.},
  langid =       {english},
  keywords =     {Humanities and Social
                  Sciences,multidisciplinary,Science},
  year =         {1986}
}

@online{raffelExploringLimitsTransfer2019,
  title =        {Exploring the {{Limits}} of {{Transfer Learning}}
                  with a {{Unified Text-to-Text Transformer}}},
  author =       {Raffel, Colin and Shazeer, Noam and Roberts, Adam
                  and Lee, Katherine and Narang, Sharan and Matena,
                  Michael and Zhou, Yanqi and Li, Wei and Liu, Peter
                  J.},
  date =         {2019-10-23},
  url =          {https://arxiv.org/abs/1910.10683v4},
  urldate =      {2025-12-04},
  abstract =     {Transfer learning, where a model is first
                  pre-trained on a data-rich task before being
                  fine-tuned on a downstream task, has emerged as a
                  powerful technique in natural language processing
                  (NLP). The effectiveness of transfer learning has
                  given rise to a diversity of approaches,
                  methodology, and practice. In this paper, we explore
                  the landscape of transfer learning techniques for
                  NLP by introducing a unified framework that converts
                  all text-based language problems into a text-to-text
                  format. Our systematic study compares pre-training
                  objectives, architectures, unlabeled data sets,
                  transfer approaches, and other factors on dozens of
                  language understanding tasks. By combining the
                  insights from our exploration with scale and our new
                  ``Colossal Clean Crawled Corpus'', we achieve
                  state-of-the-art results on many benchmarks covering
                  summarization, question answering, text
                  classification, and more. To facilitate future work
                  on transfer learning for NLP, we release our data
                  set, pre-trained models, and code.},
  langid =       {english},
  organization = {arXiv.org}
}

@article{turingComputableNumbersApplication1937,
  title =        {On {{Computable Numbers}}, with an {{Application}}
                  to the {{Entscheidungsproblem}}},
  author =       {Turing, A. M.},
  date =         {1937},
  journaltitle = {Proceedings of the London Mathematical Society},
  volume =       {s2-42},
  number =       {1},
  pages =        {230--265},
  issn =         {1460-244X},
  doi =          {10.1112/plms/s2-42.1.230},
  url =
                  {https://onlinelibrary.wiley.com/doi/abs/10.1112/plms/s2-42.1.230},
  urldate =      {2025-12-13},
  langid =       {english},
  year =         {1937}
}

@article{hochreiter_long_1997,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997}
}

@inproceedings{strubell_energy_2019,
  title={Energy and Policy Considerations for Deep Learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3645--3650},
  year={2019}
}

@article{bengio_learning_1994,
  title={Learning long-term dependencies with gradient descent is
                  difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994}
}

@article{fullerDigitalTwinEnabling2020,
  title =        {Digital {{Twin}}: {{Enabling Technologies}},
                  {{Challenges}} and {{Open Research}}},
  shorttitle =   {Digital {{Twin}}},
  author =       {Fuller, Aidan and Fan, Zhong and Day, Charles and
                  Barlow, Chris},
  date =         {2020},
  journaltitle = {IEEE Access},
  volume =       {8},
  pages =        {108952--108971},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2020.2998358},
  url =          {https://ieeexplore.ieee.org/document/9103025},
  urldate =      {2025-12-17},
  abstract =     {Digital Twin technology is an emerging concept that
                  has become the centre of attention for industry and,
                  in more recent years, academia. The advancements in
                  industry 4.0 concepts have facilitated its growth,
                  particularly in the manufacturing industry. The
                  Digital Twin is defined extensively but is best
                  described as the effortless integration of data
                  between a physical and virtual machine in either
                  direction. The challenges, applications, and
                  enabling technologies for Artificial Intelligence,
                  Internet of Things (IoT) and Digital Twins are
                  presented. A review of publications relating to
                  Digital Twins is performed, producing a categorical
                  review of recent papers. The review has categorised
                  them by research areas: manufacturing, healthcare
                  and smart cities, discussing a range of papers that
                  reflect these areas and the current state of
                  research. The paper provides an assessment of the
                  enabling technologies, challenges and open research
                  for Digital Twins.},
  keywords =     {applications,Computational modeling,Data
                  analysis,Data models,deep learning,Digital
                  twins,enabling technologies,industrial Internet of
                  Things (IIoT),Internet of Things,Internet of Things
                  (IoT),literature review,machine
                  learning,Manufacturing,Smart cities}
}

@article{gallegoEventBasedVisionSurvey2022,
  title =        {Event-{{Based Vision}}: {{A Survey}}},
  shorttitle =   {Event-{{Based Vision}}},
  author =       {Gallego, Guillermo and Delbrück, Tobi and Orchard,
                  Garrick and Bartolozzi, Chiara and Taba, Brian and
                  Censi, Andrea and Leutenegger, Stefan and Davison,
                  Andrew J. and Conradt, Jörg and Daniilidis, Kostas
                  and Scaramuzza, Davide},
  date =         {2022-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  volume =       {44},
  number =       {1},
  pages =        {154--180},
  issn =         {1939-3539},
  doi =          {10.1109/TPAMI.2020.3008413},
  url =          {https://ieeexplore.ieee.org/document/9138762},
  urldate =      {2025-12-17},
  abstract =     {Event cameras are bio-inspired sensors that differ
                  from conventional frame cameras: Instead of
                  capturing images at a fixed rate, they
                  asynchronously measure per-pixel brightness changes,
                  and output a stream of events that encode the time,
                  location and sign of the brightness changes. Event
                  cameras offer attractive properties compared to
                  traditional cameras: high temporal resolution (in
                  the order of μμs), very high dynamic range (140 dB
                  versus 60 dB), low power consumption, and high pixel
                  bandwidth (on the order of kHz) resulting in reduced
                  motion blur. Hence, event cameras have a large
                  potential for robotics and computer vision in
                  challenging scenarios for traditional cameras, such
                  as low-latency, high speed, and high dynamic range.
                  However, novel methods are required to process the
                  unconventional output of these sensors in order to
                  unlock their potential. This paper provides a
                  comprehensive overview of the emerging field of
                  event-based vision, with a focus on the applications
                  and the algorithms developed to unlock the
                  outstanding properties of event cameras. We present
                  event cameras from their working principle, the
                  actual sensors that are available and the tasks that
                  they have been used for, from low-level vision
                  (feature detection and tracking, optic flow, etc.)
                  to high-level vision (reconstruction, segmentation,
                  recognition). We also discuss the techniques
                  developed to process events, including
                  learning-based techniques, as well as specialized
                  processors for these novel sensors, such as spiking
                  neural networks. Additionally, we highlight the
                  challenges that remain to be tackled and the
                  opportunities that lie ahead in the search for a
                  more efficient, bio-inspired way for machines to
                  perceive and interact with the world.},
  keywords =     {asynchronous sensor,bio-inspired
                  vision,Brightness,Cameras,Event cameras,high dynamic
                  range,low latency,low power,Retina,Robot vision
                  systems,Voltage control}
}

@article{elnaggarProtTransUnderstandingLanguage2022,
  title =        {{{ProtTrans}}: {{Toward Understanding}} the
                  {{Language}} of {{Life Through Self-Supervised
                  Learning}}},
  shorttitle =   {{{ProtTrans}}},
  author =       {Elnaggar, Ahmed and Heinzinger, Michael and Dallago,
                  Christian and Rehawi, Ghalia and Wang, Yu and Jones,
                  Llion and Gibbs, Tom and Feher, Tamas and Angerer,
                  Christoph and Steinegger, Martin and Bhowmik,
                  Debsindhu and Rost, Burkhard},
  date =         {2022-10},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  volume =       {44},
  number =       {10},
  pages =        {7112--7127},
  issn =         {1939-3539},
  doi =          {10.1109/TPAMI.2021.3095381},
  url =          {https://ieeexplore.ieee.org/document/9477085},
  urldate =      {2025-12-17},
  abstract =     {Computational biology and bioinformatics provide
                  vast data gold-mines from protein sequences, ideal
                  for Language Models (LMs) taken from Natural
                  Language Processing (NLP). These LMs reach for new
                  prediction frontiers at low inference costs. Here,
                  we trained two auto-regressive models
                  (Transformer-XL, XLNet) and four auto-encoder models
                  (BERT, Albert, Electra, T5) on data from UniRef and
                  BFD containing up to 393 billion amino acids. The
                  protein LMs (pLMs) were trained on the Summit
                  supercomputer using 5616 GPUs and TPU Pod up-to 1024
                  cores. Dimensionality reduction revealed that the
                  raw pLM-embeddings from unlabeled data captured some
                  biophysical features of protein sequences. We
                  validated the advantage of using the embeddings as
                  exclusive input for several subsequent tasks: (1) a
                  per-residue (per-token) prediction of protein
                  secondary structure (3-state accuracy Q3=81\%-87\%);
                  (2) per-protein (pooling) predictions of protein
                  sub-cellular location (ten-state accuracy: Q10=81\%)
                  and membrane versus water-soluble (2-state accuracy
                  Q2=91\%). For secondary structure, the most
                  informative embeddings (ProtT5) for the first time
                  outperformed the state-of-the-art without multiple
                  sequence alignments (MSAs) or evolutionary
                  information thereby bypassing expensive database
                  searches. Taken together, the results implied that
                  pLMs learned some of the grammar of the language of
                  life. All our models are available through
                  https://github.com/agemagician/ProtTrans.},
  keywords =     {Amino acids,Computational biology,Computational
                  modeling,Databases,deep learning,high performance
                  computing,language modeling,machine
                  learning,Proteins,Task analysis,Three-dimensional
                  displays,Training}
}

@article{tjoaSurveyExplainableArtificial2021,
  title =        {A {{Survey}} on {{Explainable Artificial
                  Intelligence}} ({{XAI}}): {{Toward Medical XAI}}},
  shorttitle =   {A {{Survey}} on {{Explainable Artificial
                  Intelligence}} ({{XAI}})},
  author =       {Tjoa, Erico and Guan, Cuntai},
  date =         {2021-11},
  journaltitle = {IEEE Transactions on Neural Networks and Learning
                  Systems},
  volume =       {32},
  number =       {11},
  pages =        {4793--4813},
  issn =         {2162-2388},
  doi =          {10.1109/TNNLS.2020.3027314},
  url =          {https://ieeexplore.ieee.org/document/9233366},
  urldate =      {2025-12-17},
  abstract =     {Recently, artificial intelligence and machine
                  learning in general have demonstrated remarkable
                  performances in many tasks, from image processing to
                  natural language processing, especially with the
                  advent of deep learning (DL). Along with research
                  progress, they have encroached upon many different
                  fields and disciplines. Some of them require high
                  level of accountability and thus transparency, for
                  example, the medical sector. Explanations for
                  machine decisions and predictions are thus needed to
                  justify their reliability. This requires greater
                  interpretability, which often means we need to
                  understand the mechanism underlying the algorithms.
                  Unfortunately, the blackbox nature of the DL is
                  still unresolved, and many machine decisions are
                  still poorly understood. We provide a review on
                  interpretabilities suggested by different research
                  works and categorize them. The different categories
                  show different dimensions in interpretability
                  research, from approaches that provide “obviously”
                  interpretable information to the studies of complex
                  patterns. By applying the same categorization to
                  interpretability in medical research, it is hoped
                  that: 1) clinicians and practitioners can
                  subsequently approach these methods with caution; 2)
                  insight into interpretability will be born with more
                  considerations for medical practices; and 3)
                  initiatives to push forward data-based,
                  mathematically grounded, and technically grounded
                  medical education are encouraged.},
  keywords =     {Artificial intelligence,Explainable artificial
                  intelligence (XAI),interpretability,Machine
                  learning,machine learning (ML),Machine learning
                  algorithms,medical information system,Medical
                  information systems,survey}
}

@article{yurtseverSurveyAutonomousDriving2020,
  title =        {A {{Survey}} of {{Autonomous Driving}}: {{Common
                  Practices}} and {{Emerging Technologies}}},
  shorttitle =   {A {{Survey}} of {{Autonomous Driving}}},
  author =       {Yurtsever, Ekim and Lambert, Jacob and Carballo,
                  Alexander and Takeda, Kazuya},
  date =         {2020},
  journaltitle = {IEEE Access},
  volume =       {8},
  pages =        {58443--58469},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2020.2983149},
  url =          {https://ieeexplore.ieee.org/document/9046805},
  urldate =      {2025-12-17},
  abstract =     {Automated driving systems (ADSs) promise a safe,
                  comfortable and efficient driving experience.
                  However, fatalities involving vehicles equipped with
                  ADSs are on the rise. The full potential of ADSs
                  cannot be realized unless the robustness of
                  state-of-the-art is improved further. This paper
                  discusses unsolved problems and surveys the
                  technical aspect of automated driving. Studies
                  regarding present challenges, high-level system
                  architectures, emerging methodologies and core
                  functions including localization, mapping,
                  perception, planning, and human machine interfaces,
                  were thoroughly reviewed. Furthermore, many
                  state-of-the-art algorithms were implemented and
                  compared on our own platform in a real-world driving
                  setting. The paper concludes with an overview of
                  available datasets and tools for ADS development.},
  keywords =     {Accidents,automation,Automation,Autonomous
                  vehicles,control,intelligent transportation
                  systems,intelligent vehicles,Planning,Robot sensing
                  systems,robotics,Systems architecture,Task
                  analysis,Vehicle dynamics}
}

@article{shafiqueInternetThingsIoT2020,
  title =        {Internet of {{Things}} ({{IoT}}) for
                  {{Next-Generation Smart Systems}}: {{A Review}} of
                  {{Current Challenges}}, {{Future Trends}} and
                  {{Prospects}} for {{Emerging 5G-IoT Scenarios}}},
  shorttitle =   {Internet of {{Things}} ({{IoT}}) for
                  {{Next-Generation Smart Systems}}},
  author =       {Shafique, Kinza and Khawaja, Bilal A. and Sabir,
                  Farah and Qazi, Sameer and Mustaqim, Muhammad},
  date =         {2020},
  journaltitle = {IEEE Access},
  volume =       {8},
  pages =        {23022--23040},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2020.2970118},
  url =          {https://ieeexplore.ieee.org/document/8972389},
  urldate =      {2025-12-17},
  abstract =     {The Internet of Things (IoT)-centric concepts like
                  augmented reality, high-resolution video streaming,
                  self-driven cars, smart environment, e-health care,
                  etc. have a ubiquitous presence now. These
                  applications require higher data-rates, large
                  bandwidth, increased capacity, low latency and high
                  throughput. In light of these emerging concepts, IoT
                  has revolutionized the world by providing seamless
                  connectivity between heterogeneous networks
                  (HetNets). The eventual aim of IoT is to introduce
                  the plug and play technology providing the end-user,
                  ease of operation, remotely access control and
                  configurability. This paper presents the IoT
                  technology from a bird's eye view covering its
                  statistical/architectural trends, use cases,
                  challenges and future prospects. The paper also
                  presents a detailed and extensive overview of the
                  emerging 5G-IoT scenario. Fifth Generation (5G)
                  cellular networks provide key enabling technologies
                  for ubiquitous deployment of the IoT technology.
                  These include carrier aggregation, multiple-input
                  multiple-output (MIMO), massive-MIMO (M-MIMO),
                  coordinated multipoint processing (CoMP),
                  device-to-device (D2D) communications, centralized
                  radio access network (CRAN), software-defined
                  wireless sensor networking (SD-WSN), network
                  function virtualization (NFV) and cognitive radios
                  (CRs). This paper presents an exhaustive review for
                  these key enabling technologies and also discusses
                  the new emerging use cases of 5G-IoT driven by the
                  advances in artificial intelligence, machine and
                  deep learning, ongoing 5G initiatives, quality of
                  service (QoS) requirements in 5G and its
                  standardization issues. Finally, the paper discusses
                  challenges in the implementation of 5G-IoT due to
                  high data-rates requiring both cloud-based platforms
                  and IoT devices based edge computing.},
  keywords =     {5G,5G mobile communication,carrier
                  aggregation,CoMP,CRAN,CRs,HetNets,Internet of
                  Things,Internet of Things (IoT),M-MIMO,Market
                  research,MIMO,Next generation
                  networking,NFV,Protocols,QoS,Quality of
                  service,SD-WSN,Security}
}

@article{amaniGoogleEarthEngine2020,
  title =        {Google {{Earth Engine Cloud Computing Platform}} for
                  {{Remote Sensing Big Data Applications}}: {{A
                  Comprehensive Review}}},
  shorttitle =   {Google {{Earth Engine Cloud Computing Platform}} for
                  {{Remote Sensing Big Data Applications}}},
  author =       {Amani, Meisam and Ghorbanian, Arsalan and Ahmadi,
                  Seyed Ali and Kakooei, Mohammad and Moghimi, Armin
                  and Mirmazloumi, S. Mohammad and Moghaddam, Sayyed
                  Hamed Alizadeh and Mahdavi, Sahel and Ghahremanloo,
                  Masoud and Parsian, Saeid and Wu, Qiusheng and
                  Brisco, Brian},
  date =         {2020},
  journaltitle = {IEEE Journal of Selected Topics in Applied Earth
                  Observations and Remote Sensing},
  volume =       {13},
  pages =        {5326--5350},
  issn =         {2151-1535},
  doi =          {10.1109/JSTARS.2020.3021052},
  url =          {https://ieeexplore.ieee.org/document/9184118},
  urldate =      {2025-12-17},
  abstract =     {Remote sensing (RS) systems have been collecting
                  massive volumes of datasets for decades, managing
                  and analyzing of which are not practical using
                  common software packages and desktop computing
                  resources. In this regard, Google has developed a
                  cloud computing platform, called Google Earth Engine
                  (GEE), to effectively address the challenges of big
                  data analysis. In particular, this platform
                  facilitates processing big geo data over large areas
                  and monitoring the environment for long periods of
                  time. Although this platform was launched in 2010
                  and has proved its high potential for different
                  applications, it has not been fully investigated and
                  utilized for RS applications until recent years.
                  Therefore, this study aims to comprehensively
                  explore different aspects of the GEE platform,
                  including its datasets, functions,
                  advantages/limitations, and various applications.
                  For this purpose, 450 journal articles published in
                  150 journals between January 2010 and May 2020 were
                  studied. It was observed that Landsat and Sentinel
                  datasets were extensively utilized by GEE users.
                  Moreover, supervised machine learning algorithms,
                  such as Random Forest, were more widely applied to
                  image classification tasks. GEE has also been
                  employed in a broad range of applications, such as
                  Land Cover/land Use classification, hydrology, urban
                  planning, natural disaster, climate analyses, and
                  image processing. It was generally observed that the
                  number of GEE publications have significantly
                  increased during the past few years, and it is
                  expected that GEE will be utilized by more users
                  from different fields to resolve their big data
                  processing challenges.},
  keywords =     {Artificial satellites,Big data,Big Data,cloud
                  computing,Cloud
                  computing,Earth,Engines,Google,Google Earth Engine
                  (GEE),Remote sensing,remote sensing (RS)}
}

@article{sharmaMachineLearningApplications2021,
  title =        {Machine {{Learning Applications}} for {{Precision
                  Agriculture}}: {{A Comprehensive Review}}},
  shorttitle =   {Machine {{Learning Applications}} for {{Precision
                  Agriculture}}},
  author =       {Sharma, Abhinav and Jain, Arpit and Gupta, Prateek
                  and Chowdary, Vinay},
  date =         {2021},
  journaltitle = {IEEE Access},
  volume =       {9},
  pages =        {4843--4873},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2020.3048415},
  url =          {https://ieeexplore.ieee.org/document/9311735},
  urldate =      {2025-12-17},
  abstract =     {Agriculture plays a vital role in the economic
                  growth of any country. With the increase of
                  population, frequent changes in climatic conditions
                  and limited resources, it becomes a challenging task
                  to fulfil the food requirement of the present
                  population. Precision agriculture also known as
                  smart farming have emerged as an innovative tool to
                  address current challenges in agricultural
                  sustainability. The mechanism that drives this
                  cutting edge technology is machine learning (ML). It
                  gives the machine ability to learn without being
                  explicitly programmed. ML together with IoT
                  (Internet of Things) enabled farm machinery are key
                  components of the next agriculture revolution. In
                  this article, authors present a systematic review of
                  ML applications in the field of agriculture. The
                  areas that are focused are prediction of soil
                  parameters such as organic carbon and moisture
                  content, crop yield prediction, disease and weed
                  detection in crops and species detection. ML with
                  computer vision are reviewed for the classification
                  of a different set of crop images in order to
                  monitor the crop quality and yield assessment. This
                  approach can be integrated for enhanced livestock
                  production by predicting fertility patterns,
                  diagnosing eating disorders, cattle behaviour based
                  on ML models using data collected by collar sensors,
                  etc. Intelligent irrigation which includes drip
                  irrigation and intelligent harvesting techniques are
                  also reviewed that reduces human labour to a great
                  extent. This article demonstrates how
                  knowledge-based agriculture can improve the
                  sustainable productivity and quality of the
                  product.},
  keywords =     {Agricultural engineering,Agriculture,Artificial
                  intelligence,intelligent irrigation,Internet of
                  Things,IoT,Irrigation,machine
                  learning,prediction,Sensors,Soil,Wireless sensor
                  networks}
}

@article{vonruedenInformedMachineLearning2023,
  title =        {Informed {{Machine Learning}} – {{A Taxonomy}} and
                  {{Survey}} of {{Integrating Prior Knowledge}} into
                  {{Learning Systems}}},
  author =       {family=Rueden, given=Laura, prefix=von,
                  useprefix=true and Mayer, Sebastian and Beckh,
                  Katharina and Georgiev, Bogdan and Giesselbach, Sven
                  and Heese, Raoul and Kirsch, Birgit and Pfrommer,
                  Julius and Pick, Annika and Ramamurthy, Rajkumar and
                  Walczak, Michal and Garcke, Jochen and Bauckhage,
                  Christian and Schuecker, Jannis},
  date =         {2023-01},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume =       {35},
  number =       {1},
  pages =        {614--633},
  issn =         {1558-2191},
  doi =          {10.1109/TKDE.2021.3079836},
  url =          {https://ieeexplore.ieee.org/document/9429985},
  urldate =      {2025-12-17},
  abstract =     {Despite its great success, machine learning can have
                  its limits when dealing with insufficient training
                  data. A potential solution is the additional
                  integration of prior knowledge into the training
                  process which leads to the notion of informed
                  machine learning. In this paper, we present a
                  structured overview of various approaches in this
                  field. We provide a definition and propose a concept
                  for informed machine learning which illustrates its
                  building blocks and distinguishes it from
                  conventional machine learning. We introduce a
                  taxonomy that serves as a classification framework
                  for informed machine learning approaches. It
                  considers the source of knowledge, its
                  representation, and its integration into the machine
                  learning pipeline. Based on this taxonomy, we survey
                  related research and describe how different
                  knowledge representations such as algebraic
                  equations, logic rules, or simulation results can be
                  used in learning systems. This evaluation of
                  numerous papers on the basis of our taxonomy
                  uncovers key methods in the field of informed
                  machine learning.},
  keywords =     {expert knowledge,hybrid,informed,Machine
                  learning,Mathematical
                  model,neuro-symbolic,Pipelines,prior
                  knowledge,survey,Systematics,taxonomy,Taxonomy,Training,Training
                  data}
}

@article{kreuzbergerMachineLearningOperations2023,
  title =        {Machine {{Learning Operations}} ({{MLOps}}):
                  {{Overview}}, {{Definition}}, and {{Architecture}}},
  shorttitle =   {Machine {{Learning Operations}} ({{MLOps}})},
  author =       {Kreuzberger, Dominik and Kühl, Niklas and Hirschl,
                  Sebastian},
  date =         {2023},
  journaltitle = {IEEE Access},
  volume =       {11},
  pages =        {31866--31879},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2023.3262138},
  url =          {https://ieeexplore.ieee.org/document/10081336},
  urldate =      {2025-12-17},
  abstract =     {The final goal of all industrial machine learning
                  (ML) projects is to develop ML products and rapidly
                  bring them into production. However, it is highly
                  challenging to automate and operationalize ML
                  products and thus many ML endeavors fail to deliver
                  on their expectations. The paradigm of Machine
                  Learning Operations (MLOps) addresses this issue.
                  MLOps includes several aspects, such as best
                  practices, sets of concepts, and development
                  culture. However, MLOps is still a vague term and
                  its consequences for researchers and professionals
                  are ambiguous. To address this gap, we conduct
                  mixed-method research, including a literature
                  review, a tool review, and expert interviews. As a
                  result of these investigations, we contribute to the
                  body of knowledge by providing an aggregated
                  overview of the necessary principles, components,
                  and roles, as well as the associated architecture
                  and workflows. Furthermore, we provide a
                  comprehensive definition of MLOps and highlight open
                  challenges in the field. Finally, this work provides
                  guidance for ML researchers and practitioners who
                  want to automate and operate their ML products with
                  a designated set of technologies.},
  keywords =
                  {Automation,Bibliographies,CI/CD,Codes,Collaboration,DevOps,Interviews,machine
                  learning,Machine
                  learning,MLOps,operations,Training,workflow
                  orchestration}
}

@article{martin-gomezAIHigherEducation2025,
  title =        {{{AI}} in {{Higher Education}}: {{Initial Teacher
                  Training}} in the {{Critical}} and {{Didactic Use}}
                  of {{Artificial Intelligence}}},
  shorttitle =   {{{AI}} in {{Higher Education}}},
  author =       {Martín-Gómez, Sebastián and González Ruiz, Carlos
                  J.},
  date =         {2025},
  journaltitle = {IEEE Revista Iberoamericana de Tecnologias del
                  Aprendizaje},
  volume =       {20},
  pages =        {302--309},
  issn =         {1932-8540},
  doi =          {10.1109/RITA.2025.3616509},
  url =          {https://ieeexplore.ieee.org/document/11186800},
  urldate =      {2025-12-19},
  abstract =     {This study was made possible by the University of La
                  Laguna. The Educational Technology courses within
                  the Bachelor’s degrees in Early Childhood Education
                  and in Physical Activity and Sports Sciences (n =
                  103) were redesigned, integrating AI agents such as
                  ChatGPT, Gemini and Perplexity into six phases for
                  the critical use of these tools—questioning,
                  comparison, critical dialogue, verification,
                  re-elaboration and reflection—which guided students’
                  reflective interaction with AI. After the
                  intervention, a mixed 23-item questionnaire was
                  administered; descriptive statistics and thematic
                  analysis of the “Evaluation and use of AI” dimension
                  revealed that 88\% of students believe AI should be
                  didactically promoted in higher education and 42\%
                  identified ChatGPT as the agent providing the best
                  answers. The most developed competencies were
                  comparing outputs from different AI systems (66\%),
                  designing effective prompts (63\%) and critically
                  analysing responses (52\%). The main potentialities
                  highlighted were rapid access to information and
                  time saving, while perceived risks centred on
                  plagiarism and cognitive dependence. The findings
                  corroborate the pedagogical validity of the model
                  for strengthening prompt-engineering skills and
                  critical thinking, yet underscore the need to deepen
                  ethical training and rigorous verification
                  frameworks to address concerns about academic
                  integrity and AI reliability. It is concluded that a
                  reflective and regulated integration of these
                  technologies can significantly enhance teacher
                  education in higher education.},
  keywords =     {Adaptation models,artificial intelligence,Artificial
                  intelligence,Chatbots,ChatGPT,critical
                  thinking,didactic model,Education,Ethics,Generative
                  AI,higher education,Initial teacher
                  training,Planning,Reflection,Sports,Training}
}

@article{meloUsoChatGPTNo2023,
  title =        {O uso do ChatGPT no ensino de programação},
  author =       {Melo, Lafayette Batista and Moura, Thiago José
                  Marques},
  date =         {2023-12-28},
  journaltitle = {Computação Brasil},
  number =       {51},
  pages =        {43--47},
  issn =         {2965-9728},
  doi =          {10.5753/compbr.2023.51.3994},
  url =
                  {https://journals-sol.sbc.org.br/index.php/comp-br/article/view/3994},
  urldate =      {2025-12-19},
  abstract =     {O artigo aborda o uso do ChatGPT no ensino de
                  programação, explorando suas possibilidades.
                  Sugere-se que o ChatGPT pode refinar ideias e
                  fornecer insights para professores e alunos. O
                  artigo apresenta dicas para aproveitar ao máximo o
                  ChatGPT no ensino de programação como as de criação
                  de projetos e problemas a serem resolvidos, além do
                  aprendizado adaptativo e customizado, bem como o
                  desenvolvimento de exercícios e desafios.},
  langid =       {portuguese},
  language =     {pt},
  keywords =     {aprendizagem}
}

@article{xie2024integrating,
  author =       {Xie, Benjamin and Zhou, Yining and Porter, Leo},
  title =        {Integrating Generative AI into Programming
                  Education: Student Perceptions and the Challenge of
                  Correcting AI Errors},
  journal =      {ACM Transactions on Computing Education},
  url =
                  {https://link.springer.com/article/10.1007/s40593-025-00496-4},
  urldate =      {2025-12-19},
  year =         {2024},
  volume =       {24},
  number =       {3},
  pages =        {1--28},
  doi =          {10.1145/3643674}
}

@online{woodbridgeTopicDiscoveryClassification2025,
  title =        {Topic {{Discovery}} and {{Classification}} for
                  {{Responsible Generative AI Adaptation}} in {{Higher
                  Education}}},
  author =       {Woodbridge, Diane Myung-kyung and Seba, Allyson and
                  Seba, Freddie and Schwartz, Aydin},
  date =         {2025-12-17},
  eprint =       {2512.16036},
  eprinttype =   {arXiv},
  eprintclass =  {cs},
  doi =          {10.48550/arXiv.2512.16036},
  url =          {http://arxiv.org/abs/2512.16036},
  urldate =      {2025-12-19},
  abstract =     {As generative artificial intelligence (GenAI)
                  becomes increasingly capable of delivering
                  personalized learning experiences and real-time
                  feedback, a growing number of students are
                  incorporating these tools into their academic
                  workflows. They use GenAI to clarify concepts, solve
                  complex problems, and, in some cases, complete
                  assignments by copying and pasting model-generated
                  contents. While GenAI has the potential to enhance
                  learning experience, it also raises concerns around
                  misinformation, hallucinated outputs, and its
                  potential to undermine critical thinking and
                  problem-solving skills. In response, many
                  universities, colleges, departments, and instructors
                  have begun to develop and adopt policies to guide
                  responsible integration of GenAI into learning
                  environments. However, these policies vary widely
                  across institutions and contexts, and their evolving
                  nature often leaves students uncertain about
                  expectations and best practices. To address this
                  challenge, the authors designed and implemented an
                  automated system for discovering and categorizing
                  AI-related policies found in course syllabi and
                  institutional policy websites. The system combines
                  unsupervised topic modeling techniques to identify
                  key policy themes with large language models (LLMs)
                  to classify the level of GenAI allowance and other
                  requirements in policy texts. The developed
                  application achieved a coherence score of 0.73 for
                  topic discovery. In addition, GPT-4.0-based
                  classification of policy categories achieved
                  precision between 0.92 and 0.97, and recall between
                  0.85 and 0.97 across eight identified topics. By
                  providing structured and interpretable policy
                  information, this tool promotes the safe, equitable,
                  and pedagogically aligned use of GenAI technologies
                  in education. Furthermore, the system can be
                  integrated into educational technology platforms to
                  help students understand and comply with relevant
                  guidelines.},
  pubstate =     {prepublished},
  keywords =     {Computer Science - Artificial Intelligence}
}

@article{silverGeneralReinforcementLearning2018,
  title =        {A General Reinforcement Learning Algorithm That
                  Masters Chess, Shogi, and {{Go}} through Self-Play},
  author =       {Silver, David and Hubert, Thomas and Schrittwieser,
                  Julian and Antonoglou, Ioannis and Lai, Matthew and
                  Guez, Arthur and Lanctot, Marc and Sifre, Laurent
                  and Kumaran, Dharshan and Graepel, Thore and
                  Lillicrap, Timothy and Simonyan, Karen and Hassabis,
                  Demis},
  date =         {2018-12-07},
  journaltitle = {Science},
  volume =       {362},
  number =       {6419},
  pages =        {1140--1144},
  issn =         {0036-8075, 1095-9203},
  doi =          {10.1126/science.aar6404},
  url =          {https://www.science.org/doi/10.1126/science.aar6404},
  urldate =      {2026-01-11},
  abstract =     {One program to rule them all Computers can beat
                  humans at increasingly complex games, including
                  chess and Go. However, these programs are typically
                  constructed for a particular game, exploiting its
                  properties, such as the symmetries of the board on
                  which it is played. Silver et al. developed a
                  program called AlphaZero, which taught itself to
                  play Go, chess, and shogi (a Japanese version of
                  chess) (see the Editorial, and the Perspective by
                  Campbell). AlphaZero managed to beat
                  state-of-the-art programs specializing in these
                  three games. The ability of AlphaZero to adapt to
                  various game rules is a notable step toward
                  achieving a general game-playing system. Science ,
                  this issue p. 1140 ; see also pp. 1087 and 1118 ,
                  AlphaZero teaches itself to play three different
                  board games and beats state-of-the-art programs in
                  each. , The game of chess is the longest-studied
                  domain in the history of artificial intelligence.
                  The strongest programs are based on a combination of
                  sophisticated search techniques, domain-specific
                  adaptations, and handcrafted evaluation functions
                  that have been refined by human experts over several
                  decades. By contrast, the AlphaGo Zero program
                  recently achieved superhuman performance in the game
                  of Go by reinforcement learning from self-play. In
                  this paper, we generalize this approach into a
                  single AlphaZero algorithm that can achieve
                  superhuman performance in many challenging games.
                  Starting from random play and given no domain
                  knowledge except the game rules, AlphaZero
                  convincingly defeated a world champion program in
                  the games of chess and shogi (Japanese chess), as
                  well as Go.},
  langid =       {english},
  language =     {en}
}
@article{anickerWhenMachinesTake2025,
  title =        {When Machines Take over: Professional Chess as a
                  Model Case for the Societal Impact of Superhuman
                  {{AI}}},
  shorttitle =   {When Machines Take Over},
  author =       {Anicker, Fabian},
  date =         {2025-07-12},
  journaltitle = {AI \& Soc},
  issn =         {1435-5655},
  doi =          {10.1007/s00146-025-02465-w},
  url =          {https://doi.org/10.1007/s00146-025-02465-w},
  urldate =      {2026-01-11},
  abstract =     {Once emblematic of human intellectual mastery, chess
                  has become a domain where machines not only surpass
                  human ability but fundamentally reshape the
                  practice’s social dynamics, meanings, and power
                  structures. This paper examines the transformative
                  impact of superhuman AI on professional chess,
                  positioning it as an analytical model case for
                  understanding AI’s broader societal implications.
                  Drawing on a corpus of 271 h of transcribed chess
                  commentary, the analysis traces the shift from
                  symbolic AI to deep learning systems, and the
                  consequent reconfiguration of chess as a social
                  practice. The study explores how this transformation
                  alters the game’s meaning, redistributes authority,
                  reshapes power relations, and creates new social
                  roles through AI’s integration. These insights
                  foreshadow challenges for fields, such as medicine
                  or law, where AI’s ascendancy may similarly
                  redistribute authority, redefine purpose, and
                  reshape agency.},
  langid =       {english},
  language =     {en},
  keywords =     {Agency,Artificial intelligence,Chess,Social
                  field,Transformation}
}

@article{macia-lilloHybridArchitectureAIBased2025,
  title = {Hybrid {{Architecture}} for {{AI-Based RTS Games}}},
  author = {Maciá-Lillo, Antonio and Jimeno-Morenilla, Antonio and Mora, Higinio and Duta, Eduard},
  date = {2025-09},
  journaltitle = {IEEE Transactions on Games},
  volume = {17},
  number = {3},
  pages = {686--699},
  issn = {2475-1510},
  doi = {10.1109/TG.2025.3533949},
  url = {https://ieeexplore.ieee.org/document/10854898},
  urldate = {2026-01-11},
  abstract = {Video games have evolved into a key part of modern culture and a major economic force, with the global market projected to reach \$522.50 billion in 2024. As technology advances, video games increasingly demand high computing power, often requiring specialized hardware for optimal performance. Real-time strategy games, in particular, are computationally intensive, with complex artificial intelligence algorithms that simulate numerous units and behaviors in real-time. Specialized gaming PCs are use a dedicated graphics processing unit (GPU) to run video games. Due to the usefulness of GPUs besides gaming, modern processors usually include an integrated GPU, specially in the laptop market. We propose a hybrid architecture that utilizes both the dedicated GPU and the integrated GPU simultaneously, to accelerate AI and physics simulations in video games. The hybrid approach aims to maximize the utilization of all available resources. The AI and physics computations are offloaded from the dedicated GPU to the integrated GPU. Therefore, the dedicated GPU can be used exclusively for rendering, resulting in improved performance. We implemented this architecture in a custom-built game engine using OpenGL for graphics rendering and OpenCL for general-purpose GPU computations. Experimental results highlight the performance characteristics of the hybrid architecture, including the challenges of working with the two devices and multitenant GPU interference.},
  keywords = {Architecture,Artificial Intelligence,Computational Modeling,Computer Architecture,Dedicated GPU,Game Engine,Games,Graphics,Graphics Processing Units,Integrated GPU,Performance Evaluation,Physics,Portable Computers,Video Games}
}

@online{chenEvaluatingLargeLanguage2021,
  title =        {Evaluating {{Large Language Models Trained}} on
                  {{Code}}},
  author =       {Chen, Mark and Tworek, Jerry and Jun, Heewoo and
                  Yuan, Qiming and Pinto, Henrique Ponde de Oliveira
                  and Kaplan, Jared and Edwards, Harri and Burda, Yuri
                  and Joseph, Nicholas and Brockman, Greg and Ray,
                  Alex and Puri, Raul and Krueger, Gretchen and
                  Petrov, Michael and Khlaaf, Heidy and Sastry, Girish
                  and Mishkin, Pamela and Chan, Brooke and Gray, Scott
                  and Ryder, Nick and Pavlov, Mikhail and Power,
                  Alethea and Kaiser, Lukasz and Bavarian, Mohammad
                  and Winter, Clemens and Tillet, Philippe and Such,
                  Felipe Petroski and Cummings, Dave and Plappert,
                  Matthias and Chantzis, Fotios and Barnes, Elizabeth
                  and Herbert-Voss, Ariel and Guss, William Hebgen and
                  Nichol, Alex and Paino, Alex and Tezak, Nikolas and
                  Tang, Jie and Babuschkin, Igor and Balaji, Suchir
                  and Jain, Shantanu and Saunders, William and Hesse,
                  Christopher and Carr, Andrew N. and Leike, Jan and
                  Achiam, Josh and Misra, Vedant and Morikawa, Evan
                  and Radford, Alec and Knight, Matthew and Brundage,
                  Miles and Murati, Mira and Mayer, Katie and
                  Welinder, Peter and McGrew, Bob and Amodei, Dario
                  and McCandlish, Sam and Sutskever, Ilya and Zaremba,
                  Wojciech},
  date =         {2021-07-14},
  eprint =       {2107.03374},
  eprinttype =   {arXiv},
  eprintclass =  {cs},
  doi =          {10.48550/arXiv.2107.03374},
  url =          {http://arxiv.org/abs/2107.03374},
  urldate =      {2026-01-11},
  abstract =     {We introduce Codex, a GPT language model fine-tuned
                  on publicly available code from GitHub, and study
                  its Python code-writing capabilities. A distinct
                  production version of Codex powers GitHub Copilot.
                  On HumanEval, a new evaluation set we release to
                  measure functional correctness for synthesizing
                  programs from docstrings, our model solves 28.8\% of
                  the problems, while GPT-3 solves 0\% and GPT-J
                  solves 11.4\%. Furthermore, we find that repeated
                  sampling from the model is a surprisingly effective
                  strategy for producing working solutions to
                  difficult prompts. Using this method, we solve
                  70.2\% of our problems with 100 samples per problem.
                  Careful investigation of our model reveals its
                  limitations, including difficulty with docstrings
                  describing long chains of operations and with
                  binding operations to variables. Finally, we discuss
                  the potential broader impacts of deploying powerful
                  code generation technologies, covering safety,
                  security, and economics.},
  pubstate =     {prepublished},
  keywords =     {Computer Science - Machine Learning}
}

@online{dohmkeGitHubCopilotBusiness2023,
  title =        {{{GitHub Copilot}} for {{Business}} Is Now
                  Available},
  author =       {Dohmke, Thomas},
  date =         {2023-02-14T17:55:33+00:00},
  url =
                  {https://github.blog/news-insights/product-news/github-copilot-for-business-is-now-available/},
  urldate =      {2026-01-11},
  abstract =     {GitHub Copilot is the world’s first at-scale AI
                  developer tool and we’re now offering it to every
                  developer, team, organization, and enterprise.},
  langid =       {american},
  language =     {en-US},
  organization = {The GitHub Blog}
}

@book{cooperAlanTuringHis2013,
  title =        {Alan {{Turing}}: His Work and Impact},
  shorttitle =   {Alan {{Turing}}},
  author =       {Cooper, S. B. and family=Leeuwen, given=J.,
                  prefix=van, useprefix=false},
  date =         {2013},
  publisher =    {Elsevier},
  location =     {Waltham, MA : Kidlington, Oxford},
  isbn =         {978-0-12-386980-7},
  pagetotal =    {914},
  keywords =     {Biographies,Biography,Computer science,Enigma cipher
                  system,Great Britain,Logic Symbolic and
                  mathematical,Mathematicians,Mathematics,Turing Alan}
}

@article{stallmanForwardReasoningDependencydirected1977,
  title =        {Forward Reasoning and Dependency-Directed
                  Backtracking in a System for Computer-Aided Circuit
                  Analysis},
  author =       {Stallman, Richard M. and Sussman, Gerald J.},
  date =         {1977-10-01},
  journaltitle = {Artificial Intelligence},
  volume =       {9},
  number =       {2},
  pages =        {135--196},
  issn =         {0004-3702},
  doi =          {10.1016/0004-3702(77)90029-7},
  url =
                  {https://www.sciencedirect.com/science/article/pii/0004370277900297},
  urldate =      {2026-01-11},
  abstract =     {We present a rule-based system for computer-aided
                  circuit analysis. The set of rules, called EL, is
                  written in a rule language called ARS. Rules are
                  implemented by ARS as pattern-directed invocation
                  demons monitoring an associative data base.
                  Deductions are performed in an antecedent manner,
                  giving EL's analysis a catch-as-catch-can flavour
                  suggestive of the behavior of expert circuit
                  analyzers. We call this style of circuit analysis
                  propagation of constraints. The system threads
                  deduced facts with justifications which mention the
                  antecedent facts and the rule used. These
                  justifications may be examined by the user to gain
                  insight into the operation of the set of rules as
                  they apply to a problem. The same justifications are
                  used by the system to determine the currently active
                  data-base context for reasoning in hypothetical
                  situations. They are also used by the system in the
                  analysis of failures to reduce the search space.
                  This leads to effective control of combinatorial
                  search which we call dependency-directed
                  backtracking.}
}

@online{stallmanGNUManifesto1983,
  type =         {Manifesto},
  title =        {The {{GNU Manifesto}}},
  author =       {Stallman, Richard M.},
  date =         {1983},
  url =          {https://www.gnu.org/gnu/manifesto.html},
  urldate =      {2025-12-13},
  langid =       {english},
  language =     {English},
  organization = {The GNU Operating System and the Free Software
                  Movement}
}

@manual{gcc,
  title =        {Using the GNU Compiler Collection (GCC)},
  author =       {Stallman, Richard M. and others},
  organization = {Free Software Foundation},
  year =         {2025},
  url =          {https://gcc.gnu.org/onlinedocs/}
}

@article{mccarthyRecursiveFunctionsSymbolic1960,
  title =        {Recursive Functions of Symbolic Expressions and
                  Their Computation by Machine, {{Part I}}},
  author =       {McCarthy, John},
  date =         {1960-04-01},
  journaltitle = {Commun. ACM},
  volume =       {3},
  number =       {4},
  pages =        {184--195},
  issn =         {0001-0782},
  doi =          {10.1145/367177.367199},
  url =          {https://dl.acm.org/doi/10.1145/367177.367199},
  urldate =      {2025-12-13}
}

@article{yenduriArtificialGeneralIntelligence2025,
  title =        {Artificial {{General Intelligence}}:
                  {{Advancements}}, {{Challenges}}, and {{Future
                  Directions}} in {{AGI Research}}},
  shorttitle =   {Artificial {{General Intelligence}}},
  author =       {Yenduri, Gokul and Murugan, Ramalingam and Kumar
                  Reddy Maddikunta, Praveen and Bhattacharya, Sweta
                  and Sudheer, Devulapalli and Bhushan Savarala,
                  Bharath},
  date =         {2025},
  journaltitle = {IEEE Access},
  volume =       {13},
  pages =        {134325--134356},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2025.3592708},
  url =          {https://ieeexplore.ieee.org/document/11096544},
  urldate =      {2026-01-11},
  abstract =     {Artificial General Intelligence (AGI) is a
                  transformative shift in artificial intelligence that
                  aims to match human-like reasoning, flexibility, and
                  self-learning. Unlike Narrow AI, which is capable of
                  performing only a limited set of tasks, AGI aspires
                  to handle any intellectual task by exhibiting
                  human-like learning, reasoning, and behavior. This
                  enables AGI to offer extraordinary potential in
                  various domains such as healthcare, education,
                  transportation, and more. This paper provides a
                  comprehensive review of the fundamental concepts,
                  applications, and challenges associated with AGI’s
                  development. This systematic review of the
                  literature explored AGI’s transformative potential,
                  from personalized healthcare and adaptive learning
                  systems to advanced autonomous vehicles and
                  predictive analytics. Although, AGI has great
                  potential, multiple challenges, such as ethical
                  issues, data privacy, and other technical
                  challenges, must be addressed before its launch in
                  the real world.},
  keywords =     {Artificial general intelligence,artificial
                  intelligence,Artificial
                  intelligence,Cognition,Ethics,Industries,Intelligent
                  systems,machine learning,Medical services,narrow
                  AI,Problem-solving,Surveys,Systematic literature
                  review}
}

@article{yangMultimodalAISystem2022,
  title =        {A {{Multimodal AI System}} for {{Out-of-Distribution
                  Generalization}} of {{Seizure Identification}}},
  author =       {Yang, Yikai and Truong, Nhan Duy and Eshraghian,
                  Jason K. and Maher, Christina and Nikpour, Armin and
                  Kavehei, Omid},
  date =         {2022-07},
  journaltitle = {IEEE Journal of Biomedical and Health Informatics},
  volume =       {26},
  number =       {7},
  pages =        {3529--3538},
  issn =         {2168-2208},
  doi =          {10.1109/JBHI.2022.3157877},
  url =          {https://ieeexplore.ieee.org/document/9732211},
  urldate =      {2026-01-13},
  abstract =     {Artificial intelligence (AI) and health sensory
                  data-fusion hold the potential to automate many
                  laborious and time-consuming processes in hospitals
                  or ambulatory settings, e.g. home monitoring and
                  telehealth. One such unmet challenge is rapid and
                  accurate epileptic seizure annotation. An accurate
                  and automatic approach can provide an alternative
                  way to label seizures in epilepsy or deliver a
                  substitute for inaccurate patient self-reports.
                  Multimodal sensory fusion is believed to provide an
                  avenue to improve the performance of AI systems in
                  seizure identification. We propose a
                  state-of-the-art performing AI system that combines
                  electroencephalogram (EEG) and electrocardiogram
                  (ECG) for seizure identification, tested on clinical
                  data with early evidence demonstrating
                  generalization across hospitals. The model was
                  trained and validated on the publicly available
                  Temple University Hospital (TUH) dataset. To
                  evaluate performance in a clinical setting, we
                  conducted non-patient-specific pseudo-prospective
                  inference tests on three out-of-distribution
                  datasets, including EPILEPSIAE (30 patients) and the
                  Royal Prince Alfred Hospital (RPAH) in Sydney,
                  Australia (31 neurologists-shortlisted patients and
                  30 randomly selected). Our multimodal approach
                  improves the area under the receiver operating
                  characteristic curve (AUC-ROC) by an average margin
                  of 6.71\% and 14.42\% for deep learning techniques
                  using EEG-only and ECG-only, respectively. Our
                  model’s state-of-the-art performance and robustness
                  to out-of-distribution datasets show the accuracy
                  and efficiency necessary to improve epilepsy
                  diagnoses. To the best of our knowledge, this is the
                  first pseudo-prospective study of an AI system
                  combining EEG and ECG modalities for automatic
                  seizure annotation achieved with fusion of two deep
                  learning networks.},
  keywords =     {Affective computing,Artificial
                  intelligence,autonomous systems,Brain modeling,Deep
                  learning,Electrocardiography,Electroencephalography,Epilepsy,expert
                  systems,Hospitals}
}

@article{mckinneyInternationalEvaluationAI2020,
  title =        {International Evaluation of an {{AI}} System for
                  Breast Cancer Screening},
  author =       {McKinney, Scott Mayer and Sieniek, Marcin and
                  Godbole, Varun and Godwin, Jonathan and Antropova,
                  Natasha and Ashrafian, Hutan and Back, Trevor and
                  Chesus, Mary and Corrado, Greg S. and Darzi, Ara and
                  Etemadi, Mozziyar and Garcia-Vicente, Florencia and
                  Gilbert, Fiona J. and Halling-Brown, Mark and
                  Hassabis, Demis and Jansen, Sunny and
                  Karthikesalingam, Alan and Kelly, Christopher J. and
                  King, Dominic and Ledsam, Joseph R. and Melnick,
                  David and Mostofi, Hormuz and Peng, Lily and
                  Reicher, Joshua Jay and Romera-Paredes, Bernardino
                  and Sidebottom, Richard and Suleyman, Mustafa and
                  Tse, Daniel and Young, Kenneth C. and De Fauw,
                  Jeffrey and Shetty, Shravya},
  date =         {2020-01},
  journaltitle = {Nature},
  volume =       {577},
  number =       {7788},
  pages =        {89--94},
  publisher =    {Nature Publishing Group},
  issn =         {1476-4687},
  doi =          {10.1038/s41586-019-1799-6},
  url =          {https://www.nature.com/articles/s41586-019-1799-6},
  urldate =      {2026-01-13},
  abstract =     {Screening mammography aims to identify breast cancer
                  at earlier stages of the disease, when treatment can
                  be more successful1. Despite the existence of
                  screening programmes worldwide, the interpretation
                  of mammograms is affected by high rates of false
                  positives and false negatives2. Here we present an
                  artificial intelligence (AI) system that is capable
                  of surpassing human experts in breast cancer
                  prediction. To assess its performance in the
                  clinical setting, we curated a large representative
                  dataset from the UK and a large enriched dataset
                  from the USA. We show an absolute reduction of
                  5.7\%~and 1.2\% (USA and UK) in false positives and
                  9.4\%~and 2.7\% in false negatives. We provide
                  evidence of the ability of the system to generalize
                  from the UK to the USA. In an independent study of
                  six radiologists, the AI system outperformed all of
                  the human readers: the area under the receiver
                  operating characteristic curve (AUC-ROC) for the AI
                  system was greater than the AUC-ROC for the average
                  radiologist by an absolute margin of 11.5\%. We ran
                  a simulation in which the AI system participated in
                  the double-reading process that is used in the UK,
                  and found that the AI system maintained non-inferior
                  performance and reduced the workload of the second
                  reader by 88\%. This robust assessment of the AI
                  system paves the way for clinical trials to improve
                  the accuracy and efficiency of breast cancer
                  screening.},
  langid =       {english},
  keywords =     {Breast cancer,Preclinical research}
}

@article{hannunCardiologistlevelArrhythmiaDetection2019,
  title =        {Cardiologist-Level Arrhythmia Detection and
                  Classification in Ambulatory Electrocardiograms
                  Using a Deep Neural Network},
  author =       {Hannun, Awni Y. and Rajpurkar, Pranav and
                  Haghpanahi, Masoumeh and Tison, Geoffrey H. and
                  Bourn, Codie and Turakhia, Mintu P. and Ng, Andrew
                  Y.},
  date =         {2019-01},
  journaltitle = {Nat Med},
  volume =       {25},
  number =       {1},
  pages =        {65--69},
  publisher =    {Nature Publishing Group},
  issn =         {1546-170X},
  doi =          {10.1038/s41591-018-0268-3},
  url =          {https://www.nature.com/articles/s41591-018-0268-3},
  urldate =      {2026-01-13},
  abstract =     {Computerized electrocardiogram (ECG) interpretation
                  plays a critical role in the clinical ECG workflow1.
                  Widely available digital ECG data and the
                  algorithmic paradigm of deep learning2 present an
                  opportunity to substantially improve the accuracy
                  and scalability of automated ECG analysis. However,
                  a comprehensive evaluation of an end-to-end deep
                  learning approach for ECG analysis across a wide
                  variety of diagnostic classes has not been
                  previously reported. Here, we develop a deep neural
                  network (DNN) to classify 12 rhythm classes using
                  91,232 single-lead ECGs from 53,549 patients who
                  used a single-lead ambulatory ECG monitoring device.
                  When validated against an independent test dataset
                  annotated by a consensus committee of
                  board-certified practicing cardiologists, the DNN
                  achieved an average area under the receiver
                  operating characteristic curve (ROC) of 0.97. The
                  average F1 score, which is the harmonic mean of the
                  positive predictive value and sensitivity, for the
                  DNN (0.837) exceeded that of average cardiologists
                  (0.780). With specificity fixed at the average
                  specificity achieved by cardiologists, the
                  sensitivity of the DNN exceeded the average
                  cardiologist sensitivity for all rhythm classes.
                  These findings demonstrate that an end-to-end deep
                  learning approach can classify a broad range of
                  distinct arrhythmias from single-lead ECGs with high
                  diagnostic performance similar to that of
                  cardiologists. If confirmed in clinical settings,
                  this approach could reduce the rate of misdiagnosed
                  computerized ECG interpretations and improve the
                  efficiency of expert human ECG interpretation by
                  accurately triaging or prioritizing the most urgent
                  conditions.},
  langid =       {english},
  keywords =     {Arrhythmias,Machine learning}
}

@article{somaniDeepLearningElectrocardiogram2021,
  title =        {Deep Learning and the Electrocardiogram: Review of
                  the Current State-of-the-Art},
  shorttitle =   {Deep Learning and the Electrocardiogram},
  author =       {Somani, Sulaiman and Russak, Adam J and Richter,
                  Felix and Zhao, Shan and Vaid, Akhil and Chaudhry,
                  Fayzan and De Freitas, Jessica K and Naik, Nidhi and
                  Miotto, Riccardo and Nadkarni, Girish N and Narula,
                  Jagat and Argulian, Edgar and Glicksberg, Benjamin
                  S},
  date =         {2021-08-01},
  journaltitle = {Europace},
  volume =       {23},
  number =       {8},
  pages =        {1179--1191},
  issn =         {1099-5129},
  doi =          {10.1093/europace/euaa377},
  url =          {https://doi.org/10.1093/europace/euaa377},
  urldate =      {2026-01-13},
  abstract =     {In the recent decade, deep learning, a subset of
                  artificial intelligence and machine learning, has
                  been used to identify patterns in big healthcare
                  datasets for disease phenotyping, event predictions,
                  and complex decision making. Public datasets for
                  electrocardiograms (ECGs) have existed since the
                  1980s and have been used for very specific tasks in
                  cardiology, such as arrhythmia, ischemia, and
                  cardiomyopathy detection. Recently, private
                  institutions have begun curating large ECG databases
                  that are orders of magnitude larger than the public
                  databases for ingestion by deep learning models.
                  These efforts have demonstrated not only improved
                  performance and generalizability in these
                  aforementioned tasks but also application to novel
                  clinical scenarios. This review focuses on orienting
                  the clinician towards fundamental tenets of deep
                  learning, state-of-the-art prior to its use for ECG
                  analysis, and current applications of deep learning
                  on ECGs, as well as their limitations and future
                  areas of improvement.}
}

@article{mishevEvaluationSentimentAnalysis2020,
  title =        {Evaluation of {{Sentiment Analysis}} in {{Finance}}:
                  {{From Lexicons}} to {{Transformers}}},
  shorttitle =   {Evaluation of {{Sentiment Analysis}} in {{Finance}}},
  author =       {Mishev, Kostadin and Gjorgjevikj, Ana and Vodenska,
                  Irena and Chitkushev, Lubomir T. and Trajanov,
                  Dimitar},
  date =         {2020},
  journaltitle = {IEEE Access},
  volume =       {8},
  pages =        {131662--131682},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2020.3009626},
  url =          {https://ieeexplore.ieee.org/document/9142175},
  urldate =      {2026-01-13},
  abstract =     {Financial and economic news is continuously
                  monitored by financial market participants.
                  According to the efficient market hypothesis, all
                  past information is reflected in stock prices and
                  new information is instantaneously absorbed in
                  determining future stock prices. Hence, prompt
                  extraction of positive or negative sentiments from
                  news is very important for investment
                  decision-making by traders, portfolio managers and
                  investors. Sentiment analysis models can provide an
                  efficient method for extracting actionable signals
                  from the news. However, financial sentiment analysis
                  is challenging due to domain-specific language and
                  unavailability of large labeled datasets. General
                  sentiment analysis models are ineffective when
                  applied to specific domains such as finance. To
                  overcome these challenges, we design an evaluation
                  platform which we use to assess the effectiveness
                  and performance of various sentiment analysis
                  approaches, based on combinations of text
                  representation methods and machine-learning
                  classifiers. We perform more than one hundred
                  experiments using publicly available datasets,
                  labeled by financial experts. We start the
                  evaluation with specific lexicons for sentiment
                  analysis in finance and gradually build the study to
                  include word and sentence encoders, up to the latest
                  available NLP transformers. The results show
                  improved efficiency of contextual embeddings in
                  sentiment analysis compared to lexicons and fixed
                  word and sentence encoders, even when large datasets
                  are not available. Furthermore, distilled versions
                  of NLP transformers produce comparable results to
                  their larger teacher models, which makes them
                  suitable for use in production environments.},
  keywords =     {Analytical
                  models,deep-learning,Dictionaries,encoders,Feature
                  extraction,finance,Machine learning,natural language
                  processing,Semantics,sentence embedding,Sentiment
                  analysis,survey,text
                  representations,transfer-learning,transformers,word
                  embedding}
}

@article{zhangImpactArtificialIntelligence2020,
  title =        {The {{Impact}} of {{Artificial Intelligence}} and
                  {{Blockchain}} on the {{Accounting Profession}}},
  author =       {Zhang, Yingying and Xiong, Feng and Xie, Yi and Fan,
                  Xuan and Gu, Haifeng},
  date =         {2020},
  journaltitle = {IEEE Access},
  volume =       {8},
  pages =        {110461--110477},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2020.3000505},
  url =          {https://ieeexplore.ieee.org/document/9110603},
  urldate =      {2026-01-13},
  abstract =     {Recent developments in technology have introduced
                  dramatic changes to the practice of the accounting
                  profession. This paper provides a comprehensive
                  review of current developments in big data, machine
                  learning, artificial intelligence, and blockchain
                  utilized in general business practice and by
                  specialized practitioners in the accounting
                  profession worldwide. This paper explores the
                  evolution of the accounting profession following
                  these recent technological developments and assesses
                  the impact of future developments. Inherent
                  challenges and opportunities posed by these new
                  technologies pertaining to accounting professionals
                  and accounting educators are also examined,
                  including an increased demand for IT professionals
                  with accounting experience as opposed to accounting
                  major graduates. Considering the dramatic changes
                  and developments of AI applications in accounting,
                  this paper reflects how all these technologies and
                  the associated requirements of job candidates will
                  affect the desired capabilities of accounting
                  graduates and provides further discussion regarding
                  what higher institutions and their accounting
                  graduates can do to adopt such changes.},
  keywords =     {Accounting profession,artificial intelligence,big
                  data,Big Data,blockchain,Blockchain,Finance,machine
                  learning,Machine learning}
}
@article{ferreiraArtificialIntelligenceApplied2021,
  title =        {Artificial {{Intelligence Applied}} to {{Stock
                  Market Trading}}: {{A Review}}},
  shorttitle =   {Artificial {{Intelligence Applied}} to {{Stock
                  Market Trading}}},
  author =       {Ferreira, Fernando G. D. C. and Gandomi, Amir H. and
                  Cardoso, Rodrigo T. N.},
  date =         {2021},
  journaltitle = {IEEE Access},
  volume =       {9},
  pages =        {30898--30917},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2021.3058133},
  url =          {https://ieeexplore.ieee.org/document/9350582},
  urldate =      {2026-01-13},
  abstract =     {The application of Artificial Intelligence (AI) to
                  financial investment is a research area that has
                  attracted extensive research attention since the
                  1990s, when there was an accelerated technological
                  development and popularization of the personal
                  computer. Since then, countless approaches have been
                  proposed to deal with the problem of price
                  prediction in the stock market. This paper presents
                  a systematic review of the literature on Artificial
                  Intelligence applied to investments in the stock
                  market based on a sample of 2326 papers from the
                  Scopus website between 1995 and 2019. These papers
                  were divided into four categories: portfolio
                  optimization, stock market prediction using AI,
                  financial sentiment analysis, and combinations
                  involving two or more approaches. For each category,
                  the initial introductory research to its
                  state-of-the-art applications are described. In
                  addition, an overview of the review leads to the
                  conclusion that this research area is gaining
                  continuous attention and the literature is becoming
                  increasingly specific and thorough.},
  keywords =     {algotradings,artificial intelligence,Artificial
                  intelligence,Computational
                  finance,finance,Finance,Investment,Optimization,Portfolios,Stock
                  markets}
}

@article{mridhaComprehensiveReviewFake2021,
  title =        {A {{Comprehensive Review}} on {{Fake News Detection
                  With Deep Learning}}},
  author =       {Mridha, M. F. and Keya, Ashfia Jannat and Hamid, Md.
                  Abdul and Monowar, Muhammad Mostafa and Rahman, Md.
                  Saifur},
  date =         {2021},
  journaltitle = {IEEE Access},
  volume =       {9},
  pages =        {156151--156170},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2021.3129329},
  url =          {https://ieeexplore.ieee.org/document/9620068},
  urldate =      {2026-01-13},
  abstract =     {A protuberant issue of the present time is that,
                  organizations from different domains are struggling
                  to obtain effective solutions for detecting
                  online-based fake news. It is quite
                  thought-provoking to distinguish fake information on
                  the internet as it is often written to deceive
                  users. Compared with many machine learning
                  techniques, deep learning-based techniques are
                  capable of detecting fake news more accurately.
                  Previous review papers were based on data mining and
                  machine learning techniques, scarcely exploring the
                  deep learning techniques for fake news detection.
                  However, emerging deep learning-based approaches
                  such as Attention, Generative Adversarial Networks,
                  and Bidirectional Encoder Representations for
                  Transformers are absent from previous surveys. This
                  study attempts to investigate advanced and
                  state-of-the-art fake news detection mechanisms
                  pensively. We begin with highlighting the fake news
                  consequences. Then, we proceed with the discussion
                  on the dataset used in previous research and their
                  NLP techniques. A comprehensive overview of deep
                  learning-based techniques has been bestowed to
                  organize representative methods into various
                  categories. The prominent evaluation metrics in fake
                  news detection are also discussed. Nevertheless, we
                  suggest further recommendations to improve fake news
                  detection mechanisms in future research directions.},
  keywords =     {Convolutional neural networks,deep learning,Deep
                  learning,fake news,Feature extraction,machine
                  learning,Machine learning,Natural language
                  processing,Social networking (online),Terminology}
}

@article{jiangNovelStackingApproach2021,
  title =        {A {{Novel Stacking Approach}} for {{Accurate
                  Detection}} of {{Fake News}}},
  author =       {Jiang, Tao and Li, Jian Ping and Haq, Amin Ul and
                  Saboor, Abdus and Ali, Amjad},
  date =         {2021},
  journaltitle = {IEEE Access},
  volume =       {9},
  pages =        {22626--22639},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2021.3056079},
  url =          {https://ieeexplore.ieee.org/document/9343823},
  urldate =      {2026-01-13},
  abstract =     {With the increasing popularity of social media,
                  people has changed the way they access news. News
                  online has become the major source of information
                  for people. However, much information appearing on
                  the Internet is dubious and even intended to
                  mislead. Some fake news are so similar to the real
                  ones that it is difficult for human to identify
                  them. Therefore, automated fake news detection tools
                  like machine learning and deep learning models have
                  become an essential requirement. In this paper, we
                  evaluated the performance of five machine learning
                  models and three deep learning models on two fake
                  and real news datasets of different size with hold
                  out cross validation. We also used term frequency,
                  term frequency-inverse document frequency and
                  embedding techniques to obtain text representation
                  for machine learning and deep learning models
                  respectively. To evaluate models' performance, we
                  used accuracy, precision, recall and F1-score as the
                  evaluation metrics and a corrected version of
                  McNemar's test to determine if models' performance
                  is significantly different. Then, we proposed our
                  novel stacking model which achieved testing accuracy
                  of 99.94\% and 96.05 \% respectively on the ISOT
                  dataset and KDnugget dataset. Furthermore, the
                  performance of our proposed method is high as
                  compared to baseline methods. Thus, we highly
                  recommend it for fake news detection.},
  keywords =     {Deception detection,deep learning,Deep learning,fake
                  news,Feature extraction,machine learning,Machine
                  learning,McNemar’s test,Neural networks,performance
                  evaluation,Social networking
                  (online),stacking,Stacking,Support vector machines}
}

@article{hashmiAdvancingFakeNews2024,
  title =        {Advancing {{Fake News Detection}}: {{Hybrid Deep
                  Learning With FastText}} and {{Explainable AI}}},
  shorttitle =   {Advancing {{Fake News Detection}}},
  author =       {Hashmi, Ehtesham and Yayilgan, Sule Yildirim and
                  Yamin, Muhammad Mudassar and Ali, Subhan and
                  Abomhara, Mohamed},
  date =         {2024},
  journaltitle = {IEEE Access},
  volume =       {12},
  pages =        {44462--44480},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2024.3381038},
  url =          {https://ieeexplore.ieee.org/document/10477989},
  urldate =      {2026-01-13},
  abstract =     {The widespread propagation of misinformation on
                  social media platforms poses a significant concern,
                  prompting substantial endeavors within the research
                  community to develop robust detection solutions.
                  Individuals often place unwavering trust in social
                  networks, often without discerning the origins and
                  authenticity of the information disseminated through
                  these platforms. Hence, the identification of
                  media-rich fake news necessitates an approach that
                  adeptly leverages multimedia elements and
                  effectively enhances detection accuracy. The
                  ever-changing nature of cyberspace highlights the
                  need for measures that may effectively resist the
                  spread of media-rich fake news while protecting the
                  integrity of information systems. This study
                  introduces a robust approach for fake news
                  detection, utilizing three publicly available
                  datasets: WELFake, FakeNewsNet, and
                  FakeNewsPrediction. We integrated FastText word
                  embeddings with various Machine Learning and Deep
                  Learning methods, further refining these algorithms
                  with regularization and hyperparameter optimization
                  to mitigate overfitting and promote model
                  generalization. Notably, a hybrid model combining
                  Convolutional Neural Networks and Long Short-Term
                  Memory, enriched with FastText embeddings, surpassed
                  other techniques in classification performance
                  across all datasets, registering accuracy and
                  F1-scores of 0.99, 0.97, and 0.99, respectively.
                  Additionally, we utilized state-of-the-art
                  transformer-based models such as BERT, XLNet, and
                  RoBERTa, enhancing them through hyperparameter
                  adjustments. These transformer models, surpassing
                  traditional RNN-based frameworks, excel in managing
                  syntactic nuances, thus aiding in semantic
                  interpretation. In the concluding phase, explainable
                  AI modeling was employed using Local Interpretable
                  Model-Agnostic Explanations, and Latent Dirichlet
                  Allocation to gain deeper insights into the model’s
                  decision-making process.},
  keywords =     {deep learning,Deep learning,Explainable AI,Fake
                  news,Feature extraction,interpretability
                  modeling,Long short term memory,machine
                  learning,Machine learning,Semantics,Social
                  networking (online),Text
                  processing,transformers,Transformers,word
                  embeddings}
}

@article{niMVANMultiViewAttention2021,
  title =        {{{MVAN}}: {{Multi-View Attention Networks}} for
                  {{Fake News Detection}} on {{Social Media}}},
  shorttitle =   {{{MVAN}}},
  author =       {Ni, Shiwen and Li, Jiawen and Kao, Hung-Yu},
  date =         {2021},
  journaltitle = {IEEE Access},
  volume =       {9},
  pages =        {106907--106917},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2021.3100245},
  url =          {https://ieeexplore.ieee.org/document/9497048},
  urldate =      {2026-01-13},
  abstract =     {Fake news on social media is a widespread and
                  serious problem in today's society. Existing fake
                  news detection methods focus on finding clues from
                  Long text content, such as original news articles
                  and user comments. This paper solves the problem of
                  fake news detection in more realistic scenarios.
                  Only source shot-text tweet and its retweet users
                  are provided without user comments. We develop a
                  novel neural network based model, Multi-View
                  Attention Networks (MVAN) to detect fake news and
                  provide explanations on social media. The MVAN model
                  includes text semantic attention and propagation
                  structure attention, which ensures that our model
                  can capture information and clues both of source
                  tweet content and propagation structure. In
                  addition, the two attention mechanisms in the model
                  can find key clue words in fake news texts and
                  suspicious users in the propagation structure. We
                  conduct experiments on two real-world datasets, and
                  the results demonstrate that MVAN can significantly
                  outperform state-of-the-art methods by 2.5\% in
                  accuracy on average, and produce a reasonable
                  explanation.},
  keywords =     {attention,Blogs,deep learning,Deep learning,Fake
                  news detection,Feature extraction,graph attention
                  networks,Logic gates,Mathematical model,Neural
                  networks,social media,Social networking (online)}
}

@article{mullahAdvancesMachineLearning2021,
  title =        {Advances in {{Machine Learning Algorithms}} for
                  {{Hate Speech Detection}} in {{Social Media}}: {{A
                  Review}}},
  shorttitle =   {Advances in {{Machine Learning Algorithms}} for
                  {{Hate Speech Detection}} in {{Social Media}}},
  author =       {Mullah, Nanlir Sallau and Zainon, Wan Mohd Nazmee
                  Wan},
  date =         {2021},
  journaltitle = {IEEE Access},
  volume =       {9},
  pages =        {88364--88376},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2021.3089515},
  url =          {https://ieeexplore.ieee.org/document/9455353},
  urldate =      {2026-01-13},
  abstract =     {The aim of this paper is to review machine learning
                  (ML) algorithms and techniques for hate speech
                  detection in social media (SM). Hate speech problem
                  is normally model as a text classification task. In
                  this study, we examined the basic baseline
                  components of hate speech classification using ML
                  algorithms. There are five basic baseline components
                  - data collection and exploration, feature
                  extraction, dimensionality reduction, classifier
                  selection and training, and model evaluation, were
                  reviewed. There have been improvements in ML
                  algorithms that were employed for hate speech
                  detection over time. New datasets and different
                  performance metrics have been proposed in the
                  literature. To keep the researchers informed
                  regarding these trends in the automatic detection of
                  hate speech, it calls for a comprehensive and an
                  updated state-of-the-art. The contributions of this
                  study are three-fold. First to equip the readers
                  with the necessary information on the critical steps
                  involved in hate speech detection using ML
                  algorithms. Secondly, the weaknesses and strengths
                  of each method is critically evaluated to guide
                  researchers in the algorithm choice dilemma. Lastly,
                  some research gaps and open challenges were
                  identified. The different variants of ML techniques
                  were reviewed which include classical ML, ensemble
                  approach and deep learning methods. Researchers and
                  professionals alike will benefit immensely from this
                  study.},
  keywords =     {Classification algorithms,cyber hate,deep
                  learning,Deep learning,ensemble technique,Feature
                  extraction,machine learning,Machine learning
                  algorithms,social media networks,Social networking
                  (online),Sociology,Text categorization,Text
                  classification}
}

@article{murshedDEARNNHybridDeep2022,
  title =        {{{DEA-RNN}}: {{A Hybrid Deep Learning Approach}} for
                  {{Cyberbullying Detection}} in {{Twitter Social
                  Media Platform}}},
  shorttitle =   {{{DEA-RNN}}},
  author =       {Murshed, Belal Abdullah Hezam and Abawajy, Jemal and
                  Mallappa, Suresha and Saif, Mufeed Ahmed Naji and
                  Al-Ariki, Hasib Daowd Esmail},
  date =         {2022},
  journaltitle = {IEEE Access},
  volume =       {10},
  pages =        {25857--25871},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2022.3153675},
  url =          {https://ieeexplore.ieee.org/document/9718597},
  urldate =      {2026-01-13},
  abstract =     {Cyberbullying (CB) has become increasingly prevalent
                  in social media platforms. With the popularity and
                  widespread use of social media by individuals of all
                  ages, it is vital to make social media platforms
                  safer from cyberbullying. This paper presents a
                  hybrid deep learning model, called DEA-RNN, to
                  detect CB on Twitter social media network. The
                  proposed DEA-RNN model combines Elman type Recurrent
                  Neural Networks (RNN) with an optimized Dolphin
                  Echolocation Algorithm (DEA) for fine-tuning the
                  Elman RNN’s parameters and reducing training time.
                  We evaluated DEA-RNN thoroughly utilizing a dataset
                  of 10000 tweets and compared its performance to
                  those of state-of-the-art algorithms such as
                  Bi-directional long short term memory (Bi-LSTM),
                  RNN, SVM, Multinomial Naive Bayes (MNB), Random
                  Forests (RF). The experimental results show that
                  DEA-RNN was found to be superior in all the
                  scenarios. It outperformed the considered existing
                  approaches in detecting CB on Twitter platform.
                  DEA-RNN was more efficient in scenario 3, where it
                  has achieved an average of 90.45\% accuracy, 89.52\%
                  precision, 88.98\% recall, 89.25\% F1-score, and
                  90.94\% specificity.},
  keywords =     {Blogs,Cyber-bullying,Cyberbullying,cyberbullying
                  detection,Dolphin Echolocation algorithm,Elman
                  recurrent neural networks,Feature
                  extraction,Numerical models,Recurrent neural
                  networks,short text topic modeling,social
                  media,Support vector machines,Training,tweet
                  classification}
}

@article{alatawiDetectingWhiteSupremacist2021,
  title =        {Detecting {{White Supremacist Hate Speech Using
                  Domain Specific Word Embedding With Deep Learning}}
                  and {{BERT}}},
  author =       {Alatawi, Hind S. and Alhothali, Areej M. and Moria,
                  Kawthar M.},
  date =         {2021},
  journaltitle = {IEEE Access},
  volume =       {9},
  pages =        {106363--106374},
  issn =         {2169-3536},
  doi =          {10.1109/ACCESS.2021.3100435},
  url =          {https://ieeexplore.ieee.org/document/9497095},
  urldate =      {2026-01-13},
  abstract =     {White supremacist hate speech is one of the most
                  recently observed harmful content on social media.
                  The critical influence of these radical groups is no
                  longer limited to social media and can negatively
                  affect society by promoting racial hatred and
                  violence. Traditional channels of reporting hate
                  speech have proved inadequate due to the tremendous
                  explosion of information and the implicit nature of
                  hate speech. Therefore, it is necessary to detect
                  such speech automatically and in a timely manner.
                  This research investigates the feasibility of
                  automatically detecting white supremacist hate
                  speech on Twitter using deep learning and natural
                  language processing techniques. Two deep learning
                  models are investigated in this research. The first
                  approach utilizes a bidirectional Long Short-Term
                  Memory (BiLSTM) model along with domain-specific
                  word embeddings extracted from white supremacist
                  corpus to capture the semantic of white supremacist
                  slangs and coded words. The second approach utilizes
                  one of the most recent language models, which is
                  Bidirectional Encoder Representations from
                  Transformers (BERT). The BiLSTM model achieved 0.75
                  F1-score and BERT reached a 0.80 F1-score. Both
                  models are tested on a balanced dataset combined
                  from Twitter and a Stormfront dataset compiled from
                  white supremacist forum.},
  keywords =     {BERT,Bit error rate,Blogs,deep learning,Deep
                  learning,hate speech,Media,NLP,Semantics,Social
                  networking (online),Support vector
                  machines,Twitter,white supremacist}
}

@article{fui-hoonnahGenerativeAIChatGPT2023,
  title =        {Generative {{AI}} and {{ChatGPT}}: {{Applications}},
                  Challenges, and {{AI-human}} Collaboration},
  shorttitle =   {Generative {{AI}} and {{ChatGPT}}},
  author =       {Fui-Hoon Nah, Fiona and Zheng, Ruilin and Cai,
                  Jingyuan and Siau, Keng and Chen, Langtao},
  date =         {2023-07-03},
  journaltitle = {Journal of Information Technology Case and
                  Application Research},
  volume =       {25},
  number =       {3},
  pages =        {277--304},
  publisher =    {Routledge},
  issn =         {1522-8053},
  doi =          {10.1080/15228053.2023.2233814},
  url =          {https://doi.org/10.1080/15228053.2023.2233814},
  urldate =      {2026-01-13}
}

@online{rameshZeroShotTexttoImageGeneration2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2102.12092},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2102.12092},
  url = {http://arxiv.org/abs/2102.12092},
  urldate = {2026-01-13},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@online{peeblesScalableDiffusionModels2023,
  title =        {Scalable {{Diffusion Models}} with {{Transformers}}},
  author =       {Peebles, William and Xie, Saining},
  date =         {2023-03-02},
  eprint =       {2212.09748},
  eprinttype =   {arXiv},
  eprintclass =  {cs},
  doi =          {10.48550/arXiv.2212.09748},
  url =          {http://arxiv.org/abs/2212.09748},
  urldate =      {2026-01-13},
  abstract =     {We explore a new class of diffusion models based on
                  the transformer architecture. We train latent
                  diffusion models of images, replacing the
                  commonly-used U-Net backbone with a transformer that
                  operates on latent patches. We analyze the
                  scalability of our Diffusion Transformers (DiTs)
                  through the lens of forward pass complexity as
                  measured by Gflops. We find that DiTs with higher
                  Gflops -- through increased transformer depth/width
                  or increased number of input tokens -- consistently
                  have lower FID. In addition to possessing good
                  scalability properties, our largest DiT-XL/2 models
                  outperform all prior diffusion models on the
                  class-conditional ImageNet 512x512 and 256x256
                  benchmarks, achieving a state-of-the-art FID of 2.27
                  on the latter.},
  pubstate =     {prepublished},
  keywords =     {Computer Science - Computer Vision and Pattern
                  Recognition,Computer Science - Machine Learning},
  note =         {NOTA: Esse é o artigo introdutório para o Sora, a
                  ferramenta de geração de vídeo da OpenAI.}
}

@online{karrasStyleBasedGeneratorArchitecture2019,
  title =        {A {{Style-Based Generator Architecture}} for
                  {{Generative Adversarial Networks}}},
  author =       {Karras, Tero and Laine, Samuli and Aila, Timo},
  date =         {2019-03-29},
  eprint =       {1812.04948},
  eprinttype =   {arXiv},
  eprintclass =  {cs},
  doi =          {10.48550/arXiv.1812.04948},
  url =          {http://arxiv.org/abs/1812.04948},
  urldate =      {2026-01-13},
  abstract =     {We propose an alternative generator architecture for
                  generative adversarial networks, borrowing from
                  style transfer literature. The new architecture
                  leads to an automatically learned, unsupervised
                  separation of high-level attributes (e.g., pose and
                  identity when trained on human faces) and stochastic
                  variation in the generated images (e.g., freckles,
                  hair), and it enables intuitive, scale-specific
                  control of the synthesis. The new generator improves
                  the state-of-the-art in terms of traditional
                  distribution quality metrics, leads to demonstrably
                  better interpolation properties, and also better
                  disentangles the latent factors of variation. To
                  quantify interpolation quality and disentanglement,
                  we propose two new, automated methods that are
                  applicable to any generator architecture. Finally,
                  we introduce a new, highly varied and high-quality
                  dataset of human faces.},
  pubstate =     {prepublished},
  keywords =     {Computer Science - Machine Learning,Computer Science
                  - Neural and Evolutionary Computing,Statistics -
                  Machine Learning}
}

@online{rombachHighResolutionImageSynthesis2022,
  title =        {High-{{Resolution Image Synthesis}} with {{Latent
                  Diffusion Models}}},
  author =       {Rombach, Robin and Blattmann, Andreas and Lorenz,
                  Dominik and Esser, Patrick and Ommer, Björn},
  date =         {2022-04-13},
  eprint =       {2112.10752},
  eprinttype =   {arXiv},
  eprintclass =  {cs},
  doi =          {10.48550/arXiv.2112.10752},
  url =          {http://arxiv.org/abs/2112.10752},
  urldate =      {2026-01-14},
  abstract =     {By decomposing the image formation process into a
                  sequential application of denoising autoencoders,
                  diffusion models (DMs) achieve state-of-the-art
                  synthesis results on image data and beyond.
                  Additionally, their formulation allows for a guiding
                  mechanism to control the image generation process
                  without retraining. However, since these models
                  typically operate directly in pixel space,
                  optimization of powerful DMs often consumes hundreds
                  of GPU days and inference is expensive due to
                  sequential evaluations. To enable DM training on
                  limited computational resources while retaining
                  their quality and flexibility, we apply them in the
                  latent space of powerful pretrained autoencoders. In
                  contrast to previous work, training diffusion models
                  on such a representation allows for the first time
                  to reach a near-optimal point between complexity
                  reduction and detail preservation, greatly boosting
                  visual fidelity. By introducing cross-attention
                  layers into the model architecture, we turn
                  diffusion models into powerful and flexible
                  generators for general conditioning inputs such as
                  text or bounding boxes and high-resolution synthesis
                  becomes possible in a convolutional manner. Our
                  latent diffusion models (LDMs) achieve a new state
                  of the art for image inpainting and highly
                  competitive performance on various tasks, including
                  unconditional image generation, semantic scene
                  synthesis, and super-resolution, while significantly
                  reducing computational requirements compared to
                  pixel-based DMs. Code is available at
                  https://github.com/CompVis/latent-diffusion .},
  pubstate =     {prepublished},
  keywords =     {Computer Science - Computer Vision and Pattern
                  Recognition}
}

@inproceedings{abadiThisPaperIncluded,
  title = {This {{Paper Is Included}} in the {{Proceedings}} of the
                  12th {{Usenix Symposium}} on {{Operating Systems
                  Design}} and {{Implementation}} (Osdi '16).
                  {{Tensorflow}}: A {{System}} for {{Large-scale
                  Machine Learning Tensorflow}}: A {{System}} for
                  {{Large-scale Machine Learning}}},
  shorttitle = {This {{Paper Is Included}} in the {{Proceedings}} of the 12th {{Usenix Symposium}} on {{Operating Systems Design}} and {{Implementation}} (Osdi '16). {{Tensorflow}}},
  author = {Abadi, Martín and Barham, P. and Chen, Jianmin and Chen, Z. and Davis, Andy and Dean, J. and Devin, M. and Ghemawat, S. and Irving, G. and Isard, M. and Kudlur, M. and Levenberg, J. and Monga, R. and Moore, Sherry and Murray, D. and Steiner, Benoit and Tucker, P. and Vasudevan, Vijay and Warden, Pete and Wicke, M. and Yu, Yuan and Zhang, Xiaoqiang},
  url = {https://www.semanticscholar.org/paper/This-Paper-Is-Included-in-the-Proceedings-of-the-on-Abadi-Barham/4954fa180728932959997a4768411ff9136aac81?hl=pt-BR},
  urldate = {2026-01-14},
  abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous " parameter server " designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production , we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that Tensor-Flow achieves for several real-world applications.}
}

@inproceedings{45381,
  title =        {TensorFlow: A system for large-scale machine
                  learning},
  author =       {Martin Abadi and Paul Barham and Jianmin Chen and
                  Zhifeng Chen and Andy Davis and Jeffrey Dean and
                  Matthieu Devin and Sanjay Ghemawat and Geoffrey
                  Irving and Michael Isard and Manjunath Kudlur and
                  Josh Levenberg and Rajat Monga and Sherry Moore and
                  Derek G. Murray and Benoit Steiner and Paul Tucker
                  and Vijay Vasudevan and Pete Warden and Martin Wicke
                  and Yuan Yu and Xiaoqiang Zheng},
  year =         {2016},
  URL =
                  {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
  booktitle =    {12th USENIX Symposium on Operating Systems Design
                  and Implementation (OSDI 16)},
  pages =        {265--283}
}

@online{linAnimateDiffLightningCrossModelDiffusion2024,
  title =        {{{AnimateDiff-Lightning}}: {{Cross-Model Diffusion
                  Distillation}}},
  shorttitle =   {{{AnimateDiff-Lightning}}},
  author =       {Lin, Shanchuan and Yang, Xiao},
  date =         {2024-03-19},
  eprint =       {2403.12706},
  eprinttype =   {arXiv},
  eprintclass =  {cs},
  doi =          {10.48550/arXiv.2403.12706},
  url =          {http://arxiv.org/abs/2403.12706},
  urldate =      {2026-01-14},
  abstract =     {We present AnimateDiff-Lightning for lightning-fast
                  video generation. Our model uses progressive
                  adversarial diffusion distillation to achieve new
                  state-of-the-art in few-step video generation. We
                  discuss our modifications to adapt it for the video
                  modality. Furthermore, we propose to simultaneously
                  distill the probability flow of multiple base
                  diffusion models, resulting in a single distilled
                  motion module with broader style compatibility. We
                  are pleased to release our distilled
                  AnimateDiff-Lightning model for the community's
                  use.},
  pubstate =     {prepublished},
  keywords =     {Computer Science - Artificial Intelligence,Computer
                  Science - Computer Vision and Pattern Recognition}
}

@inproceedings{45381,
  title =        {TensorFlow: A system for large-scale machine
                  learning},
  author =       {Martin Abadi and Paul Barham and Jianmin Chen and
                  Zhifeng Chen and Andy Davis and Jeffrey Dean and
                  Matthieu Devin and Sanjay Ghemawat and Geoffrey
                  Irving and Michael Isard and Manjunath Kudlur and
                  Josh Levenberg and Rajat Monga and Sherry Moore and
                  Derek G. Murray and Benoit Steiner and Paul Tucker
                  and Vijay Vasudevan and Pete Warden and Martin Wicke
                  and Yuan Yu and Xiaoqiang Zheng},
  year =         {2016},
  URL =
                  {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
  booktitle =    {12th USENIX Symposium on Operating Systems Design
                  and Implementation (OSDI 16)},
  pages =        {265--283}
}

@inproceedings{paszkePyTorchImperativeStyle2019,
  title =        {{{PyTorch}}: {{An Imperative Style}},
                  {{High-Performance Deep Learning Library}}},
  shorttitle =   {{{PyTorch}}},
  booktitle =    {Advances in {{Neural Information Processing
                  Systems}}},
  author =       {Paszke, Adam and Gross, Sam and Massa, Francisco and
                  Lerer, Adam and Bradbury, James and Chanan, Gregory
                  and Killeen, Trevor and Lin, Zeming and Gimelshein,
                  Natalia and Antiga, Luca and Desmaison, Alban and
                  Kopf, Andreas and Yang, Edward and DeVito, Zachary
                  and Raison, Martin and Tejani, Alykhan and
                  Chilamkurthy, Sasank and Steiner, Benoit and Fang,
                  Lu and Bai, Junjie and Chintala, Soumith},
  date =         {2019},
  volume =       {32},
  publisher =    {Curran Associates, Inc.},
  url =
                  {https://papers.nips.cc/paper_files/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
  urldate =      {2026-01-14}
}

@online{touvronLLaMAOpenEfficient2023,
  title =        {{{LLaMA}}: {{Open}} and {{Efficient Foundation
                  Language Models}}},
  shorttitle =   {{{LLaMA}}},
  author =       {Touvron, Hugo and Lavril, Thibaut and Izacard,
                  Gautier and Martinet, Xavier and Lachaux, Marie-Anne
                  and Lacroix, Timothée and Rozière, Baptiste and
                  Goyal, Naman and Hambro, Eric and Azhar, Faisal and
                  Rodriguez, Aurelien and Joulin, Armand and Grave,
                  Edouard and Lample, Guillaume},
  date =         {2023-02-27},
  eprint =       {2302.13971},
  eprinttype =   {arXiv},
  eprintclass =  {cs},
  doi =          {10.48550/arXiv.2302.13971},
  url =          {http://arxiv.org/abs/2302.13971},
  urldate =      {2026-01-14},
  abstract =     {We introduce LLaMA, a collection of foundation
                  language models ranging from 7B to 65B parameters.
                  We train our models on trillions of tokens, and show
                  that it is possible to train state-of-the-art models
                  using publicly available datasets exclusively,
                  without resorting to proprietary and inaccessible
                  datasets. In particular, LLaMA-13B outperforms GPT-3
                  (175B) on most benchmarks, and LLaMA-65B is
                  competitive with the best models, Chinchilla-70B and
                  PaLM-540B. We release all our models to the research
                  community.},
  pubstate =     {prepublished},
  keywords =     {Computer Science - Computation and Language}
}

@online{heikkilaRadicalNewProject2022,
  title =        {Inside a Radical New Project to Democratize {{AI}}},
  author =       {Heikkilä, Melissa},
  date =         {2022-07-12},
  url =
                  {https://www.technologyreview.com/2022/07/12/1055817/inside-a-radical-new-project-to-democratize-ai/},
  urldate =      {2026-01-14},
  abstract =     {A group of over 1,000 AI researchers has created a
                  multilingual large language model bigger than
                  GPT-3—and they’re giving it out for free.},
  langid =       {english},
  organization = {MIT Technology Review},
  keywords =     {Artificial Intelligence}
}

@online{kongHunyuanVideoSystematicFramework2025,
  title =        {{{HunyuanVideo}}: {{A Systematic Framework For Large
                  Video Generative Models}}},
  shorttitle =   {{{HunyuanVideo}}},
  author =       {Kong, Weijie and Tian, Qi and Zhang, Zijian and Min,
                  Rox and Dai, Zuozhuo and Zhou, Jin and Xiong,
                  Jiangfeng and Li, Xin and Wu, Bo and Zhang, Jianwei
                  and Wu, Kathrina and Lin, Qin and Yuan, Junkun and
                  Long, Yanxin and Wang, Aladdin and Wang, Andong and
                  Li, Changlin and Huang, Duojun and Yang, Fang and
                  Tan, Hao and Wang, Hongmei and Song, Jacob and Bai,
                  Jiawang and Wu, Jianbing and Xue, Jinbao and Wang,
                  Joey and Wang, Kai and Liu, Mengyang and Li, Pengyu
                  and Li, Shuai and Wang, Weiyan and Yu, Wenqing and
                  Deng, Xinchi and Li, Yang and Chen, Yi and Cui,
                  Yutao and Peng, Yuanbo and Yu, Zhentao and He, Zhiyu
                  and Xu, Zhiyong and Zhou, Zixiang and Xu, Zunnan and
                  Tao, Yangyu and Lu, Qinglin and Liu, Songtao and
                  Zhou, Dax and Wang, Hongfa and Yang, Yong and Wang,
                  Di and Liu, Yuhong and Jiang, Jie and Zhong, Caesar},
  date =         {2025-03-11},
  eprint =       {2412.03603},
  eprinttype =   {arXiv},
  eprintclass =  {cs},
  doi =          {10.48550/arXiv.2412.03603},
  url =          {http://arxiv.org/abs/2412.03603},
  urldate =      {2026-01-14},
  abstract =     {Recent advancements in video generation have
                  significantly impacted daily life for both
                  individuals and industries. However, the leading
                  video generation models remain closed-source,
                  resulting in a notable performance gap between
                  industry capabilities and those available to the
                  public. In this report, we introduce HunyuanVideo,
                  an innovative open-source video foundation model
                  that demonstrates performance in video generation
                  comparable to, or even surpassing, that of leading
                  closed-source models. HunyuanVideo encompasses a
                  comprehensive framework that integrates several key
                  elements, including data curation, advanced
                  architectural design, progressive model scaling and
                  training, and an efficient infrastructure tailored
                  for large-scale model training and inference. As a
                  result, we successfully trained a video generative
                  model with over 13 billion parameters, making it the
                  largest among all open-source models. We conducted
                  extensive experiments and implemented a series of
                  targeted designs to ensure high visual quality,
                  motion dynamics, text-video alignment, and advanced
                  filming techniques. According to evaluations by
                  professionals, HunyuanVideo outperforms previous
                  state-of-the-art models, including Runway Gen-3,
                  Luma 1.6, and three top-performing Chinese video
                  generative models. By releasing the code for the
                  foundation model and its applications, we aim to
                  bridge the gap between closed-source and open-source
                  communities. This initiative will empower
                  individuals within the community to experiment with
                  their ideas, fostering a more dynamic and vibrant
                  video generation ecosystem. The code is publicly
                  available at
                  https://github.com/Tencent/HunyuanVideo.},
  pubstate =     {prepublished},
  keywords =     {Computer Science - Computer Vision and Pattern
                  Recognition}
}

@software{bytedanceByteDanceAnimateDiffLightningHugging2024,
  title =        {{{ByteDance}}/{{AnimateDiff-Lightning}} · {{Hugging
                  Face}}},
  author =       {{ByteDance}},
  date =         {2024-03-19},
  url =
                  {https://huggingface.co/ByteDance/AnimateDiff-Lightning},
  urldate =      {2026-01-14},
  abstract =     {AnimateDiff-Lightning is a lightning-fast
                  text-to-video generation model. It can generate
                  videos more than ten times faster than the original
                  AnimateDiff.},
  organization = {ByteDance}
}

@software{tencentTencentHunyuanVideoHugging2025,
  title =        {Tencent/{{HunyuanVideo}} · {{Hugging Face}}},
  shorttitle =   {{{HunyuanVideo}}},
  author =       {{Tencent}},
  date =         {2025-12-25},
  url =          {https://huggingface.co/tencent/HunyuanVideo},
  urldate =      {2026-01-14},
  abstract =     {We present HunyuanVideo, a novel open-source video
                  foundation model that exhibits performance in video
                  generation that is comparable to, if not superior
                  to, leading closed-source models. In order to train
                  HunyuanVideo model, we adopt several key
                  technologies for model learning, including data
                  curation, image-video joint model training, and an
                  efficient infrastructure designed to facilitate
                  large-scale model training and inference.
                  Additionally, through an effective strategy for
                  scaling model architecture and dataset, we
                  successfully trained a video generative model with
                  over 13 billion parameters, making it the largest
                  among all open-source models. We conducted extensive
                  experiments and implemented a series of targeted
                  designs to ensure high visual quality, motion
                  diversity, text-video alignment, and generation
                  stability. According to professional human
                  evaluation results, HunyuanVideo outperforms
                  previous state-of-the-art models, including Runway
                  Gen-3, Luma 1.6, and 3 top-performing Chinese video
                  generative models. By releasing the code and weights
                  of the foundation model and its applications, we aim
                  to bridge the gap between closed-source and
                  open-source video foundation models. This initiative
                  will empower everyone in the community to experiment
                  with their ideas, fostering a more dynamic and
                  vibrant video generation ecosystem.},
  organization = {Tencent}
}

@article{samekExplainingDeepNeural2021,
  title =        {Explaining {{Deep Neural Networks}} and {{Beyond}}:
                  {{A Review}} of {{Methods}} and {{Applications}}},
  shorttitle =   {Explaining {{Deep Neural Networks}} and {{Beyond}}},
  author =       {Samek, Wojciech and Montavon, Grégoire and
                  Lapuschkin, Sebastian and Anders, Christopher J. and
                  Müller, Klaus-Robert},
  date =         {2021-03},
  journaltitle = {Proceedings of the IEEE},
  volume =       {109},
  number =       {3},
  pages =        {247--278},
  issn =         {1558-2256},
  doi =          {10.1109/JPROC.2021.3060483},
  url =          {https://ieeexplore.ieee.org/document/9369420},
  urldate =      {2026-01-14},
  abstract =     {With the broader and highly successful usage of
                  machine learning (ML) in industry and the sciences,
                  there has been a growing demand for explainable
                  artificial intelligence (XAI). Interpretability and
                  explanation methods for gaining a better
                  understanding of the problem-solving abilities and
                  strategies of nonlinear ML, in particular, deep
                  neural networks, are, therefore, receiving increased
                  attention. In this work, we aim to: 1) provide a
                  timely overview of this active emerging field, with
                  a focus on “post hoc” explanations, and explain its
                  theoretical foundations; 2) put interpretability
                  algorithms to a test both from a theory and
                  comparative evaluation perspective using extensive
                  simulations; 3) outline best practice aspects, i.e.,
                  how to best include interpretation methods into the
                  standard usage of ML; and 4) demonstrate successful
                  usage of XAI in a representative selection of
                  application scenarios. Finally, we discuss
                  challenges and possible future directions of this
                  exciting foundational field of ML.},
  keywords =     {Artificial intelligence,Best practices,Black-box
                  models,deep learning,Deep learning,explainable
                  artificial intelligence
                  (XAI),Interpretability,Machine learning,model
                  transparency,neural networks,Neural
                  networks,Problem-solving,Systematics,Unsupervised
                  learning}
}

@article{hospedalesMetaLearningNeuralNetworks2022,
  title =        {Meta-{{Learning}} in {{Neural Networks}}: {{A
                  Survey}}},
  shorttitle =   {Meta-{{Learning}} in {{Neural Networks}}},
  author =       {Hospedales, Timothy and Antoniou, Antreas and
                  Micaelli, Paul and Storkey, Amos},
  date =         {2022-09},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  volume =       {44},
  number =       {9},
  pages =        {5149--5169},
  issn =         {1939-3539},
  doi =          {10.1109/TPAMI.2021.3079209},
  url =          {https://ieeexplore.ieee.org/document/9428530},
  urldate =      {2026-01-14},
  abstract =     {The field of meta-learning, or learning-to-learn,
                  has seen a dramatic rise in interest in recent
                  years. Contrary to conventional approaches to AI
                  where tasks are solved from scratch using a fixed
                  learning algorithm, meta-learning aims to improve
                  the learning algorithm itself, given the experience
                  of multiple learning episodes. This paradigm
                  provides an opportunity to tackle many conventional
                  challenges of deep learning, including data and
                  computation bottlenecks, as well as generalization.
                  This survey describes the contemporary meta-learning
                  landscape. We first discuss definitions of
                  meta-learning and position it with respect to
                  related fields, such as transfer learning and
                  hyperparameter optimization. We then propose a new
                  taxonomy that provides a more comprehensive
                  breakdown of the space of meta-learning methods
                  today. We survey promising applications and
                  successes of meta-learning such as few-shot learning
                  and reinforcement learning. Finally, we discuss
                  outstanding challenges and promising areas for
                  future research.},
  keywords =     {Deep learning,few-shot
                  learning,learning-to-learn,Machine learning
                  algorithms,Meta-learning,neural architecture
                  search,Neural networks,Optimization,Predictive
                  models,Task analysis,Training,transfer learning}
}
