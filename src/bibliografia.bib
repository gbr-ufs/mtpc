@online{cornelio_historia_2021,
  title =        {História da tecnologia: da pré-história ao
                  {Metaverso} • {Usemobile}},
  shorttitle =   {História da tecnologia},
  url =          {https://usemobile.com.br/historia-da-tecnologia/},
  abstract =     {Confira a história da tecnologia, conhecendo desde
                  os primórdios da humanidade até os adventos
                  contemporâneos.},
  language =     {pt-BR},
  urldate =      {2025-11-20},
  journal =      {Usemobile},
  author =       {Cornélio, Angelina},
  month =        nov,
  year =         {2021},
}

@online{google_global_2025,
  title =        {Global {AI} {Optimism} {Increases} as {Usage}
                  {Grows}},
  url =
                  {https://publicpolicy.google/article/global-ai-optimism-increases-as-usage-grows/},
  abstract =     {Based on the second annual global survey from Ipsos
                  and Google surveying 21,000 citizens across 21
                  countries, attitudes towards AI are trending more
                  positive as its use becomes more widespread.},
  language =     {en},
  urldate =      {2025-11-20},
  journal =      {Google Public Policy},
  author =       {Marr, Bernard},
  year =         {2025},
}

@online{marrOs15Maiores2023,
  type =         {News},
  title =        {Os 15 maiores riscos da inteligência artificial},
  author =       {Marr, Bernard},
  date =         {2023-06-05T06:00:55-03:00},
  url =
                  {https://forbes.com.br/forbes-tech/2023/06/os-15-maiores-riscos-da-inteligencia-artificial/},
  urldate =      {2025-11-16},
  abstract =     {Today we’re announcing new funding—\$40B at a \$300B
                  post-money valuation, which enables us to push the
                  frontiers of AI research even further, scale our
                  compute infrastructure, and deliver increasingly
                  powerful tools for the 500 million people who use
                  ChatGPT every week.},
  langid =       {brazilian},
  organization = {Forbes Brasil},
  year =         {2023},
}

@inproceedings{leeImpactGenerativeAI2025,
  title =        {The {{Impact}} of {{Generative AI}} on {{Critical
                  Thinking}}: {{Self-Reported Reductions}} in
                  {{Cognitive Effort}} and {{Confidence Effects From}}
                  a {{Survey}} of {{Knowledge Workers}}},
  shorttitle =   {The {{Impact}} of {{Generative AI}} on {{Critical
                  Thinking}}},
  booktitle =    {Proceedings of the 2025 {{CHI Conference}} on
                  {{Human Factors}} in {{Computing Systems}}},
  author =       {Lee, Hao-Ping (Hank) and Sarkar, Advait and
                  Tankelevitch, Lev and Drosos, Ian and Rintel, Sean
                  and Banks, Richard and Wilson, Nicholas},
  date =         {2025-04-25},
  series =       {{{CHI}} '25},
  pages =        {1--22},
  publisher =    {Association for Computing Machinery},
  location =     {New York, NY, USA},
  doi =          {10.1145/3706598.3713778},
  url =          {https://dl.acm.org/doi/10.1145/3706598.3713778},
  urldate =      {2025-11-16},
  abstract =     {The rise of Generative AI (GenAI) in knowledge
                  workflows raises questions about its impact on
                  critical thinking skills and practices. We survey
                  319 knowledge workers to investigate 1) when and how
                  they perceive the enaction of critical thinking when
                  using GenAI, and 2) when and why GenAI affects their
                  effort to do so. Participants shared 936 first-hand
                  examples of using GenAI in work tasks.
                  Quantitatively, when considering both task- and
                  user-specific factors, a user’s task-specific
                  self-confidence and confidence in GenAI are
                  predictive of whether critical thinking is enacted
                  and the effort of doing so in GenAI-assisted tasks.
                  Specifically, higher confidence in GenAI is
                  associated with less critical thinking, while higher
                  self-confidence is associated with more critical
                  thinking. Qualitatively, GenAI shifts the nature of
                  critical thinking toward information verification,
                  response integration, and task stewardship. Our
                  insights reveal new design challenges and
                  opportunities for developing GenAI tools for
                  knowledge work.},
  isbn =         {979-8-4007-1394-1},
  year =         2025,
}

@online{kosmynaYourBrainChatGPT2025,
  title =        {Your {{Brain}} on {{ChatGPT}}: {{Accumulation}} of
                  {{Cognitive Debt}} When {{Using}} an {{AI
                  Assistant}} for {{Essay Writing Task}}},
  shorttitle =   {Your {{Brain}} on {{ChatGPT}}},
  author =       {Kosmyna, Nataliya and Hauptmann, Eugene and Yuan, Ye
                  Tong and Situ, Jessica and Liao, Xian-Hao and
                  Beresnitzky, Ashly Vivian and Braunstein, Iris and
                  Maes, Pattie},
  date =         {2025-06-10},
  eprint =       {2506.08872},
  eprinttype =   {arXiv},
  eprintclass =  {cs},
  doi =          {10.48550/arXiv.2506.08872},
  url =          {http://arxiv.org/abs/2506.08872},
  urldate =      {2025-11-16},
  abstract =     {This study explores the neural and behavioral
                  consequences of LLM-assisted essay writing.
                  Participants were divided into three groups: LLM,
                  Search Engine, and Brain-only (no tools). Each
                  completed three sessions under the same condition.
                  In a fourth session, LLM users were reassigned to
                  Brain-only group (LLM-to-Brain), and Brain-only
                  users were reassigned to LLM condition
                  (Brain-to-LLM). A total of 54 participants took part
                  in Sessions 1-3, with 18 completing session 4. We
                  used electroencephalography (EEG) to assess
                  cognitive load during essay writing, and analyzed
                  essays using NLP, as well as scoring essays with the
                  help from human teachers and an AI judge. Across
                  groups, NERs, n-gram patterns, and topic ontology
                  showed within-group homogeneity. EEG revealed
                  significant differences in brain connectivity:
                  Brain-only participants exhibited the strongest,
                  most distributed networks; Search Engine users
                  showed moderate engagement; and LLM users displayed
                  the weakest connectivity. Cognitive activity scaled
                  down in relation to external tool use. In session 4,
                  LLM-to-Brain participants showed reduced alpha and
                  beta connectivity, indicating under-engagement.
                  Brain-to-LLM users exhibited higher memory recall
                  and activation of occipito-parietal and prefrontal
                  areas, similar to Search Engine users. Self-reported
                  ownership of essays was the lowest in the LLM group
                  and the highest in the Brain-only group. LLM users
                  also struggled to accurately quote their own work.
                  While LLMs offer immediate convenience, our findings
                  highlight potential cognitive costs. Over four
                  months, LLM users consistently underperformed at
                  neural, linguistic, and behavioral levels. These
                  results raise concerns about the long-term
                  educational implications of LLM reliance and
                  underscore the need for deeper inquiry into AI's
                  role in learning.},
  pubstate =     {prepublished},
  keywords =     {Computer Science - Artificial Intelligence},
  year =         {2025},
}

@misc{delikoura2025superficialoutputssuperficiallearning,
  title =        {From Superficial Outputs to Superficial Learning:
                  Risks of Large Language Models in Education},
  author =       {Iris Delikoura and Yi. R Fung and Pan Hui},
  year =         2025,
  eprint =       {2509.21972},
  archivePrefix ={arXiv},
  primaryClass = {cs.CY},
  url =          {https://arxiv.org/abs/2509.21972},
}

@online{openai_funding_2025,
  title =        {New funding to build towards {AGI}},
  url =          {https://openai.com/index/march-funding-updates/},
  abstract =     {Today we’re announcing new funding—\$40B at a \$300B
                  post-money valuation, which enables us to push the
                  frontiers of AI research even further, scale our
                  compute infrastructure, and deliver increasingly
                  powerful tools for the 500 million people who use
                  ChatGPT every week.},
  author =       {OpenAI},
  language =     {en-US},
  urldate =      {2025-11-21},
  year =         {2025},
}

@INPROCEEDINGS{10748289,
  author =       {Jia, Yuelong},
  booktitle =    {2024 3rd International Conference on Artificial
                  Intelligence, Internet of Things and Cloud Computing
                  Technology (AIoTC)},
  title =        {Analysis of the Impact of Artificial Intelligence on
                  Electricity Consumption},
  year =         2024,
  pages =        {57-60},
  keywords =     {Data centers;Electricity;Computational
                  modeling;Energy conservation;Demand response;Power
                  markets;Energy efficiency;Artificial
                  intelligence;Carbon;Uninterruptible power
                  systems;artificial intelligence;electricity
                  demand;demand response;energy efficiency},
  doi =          {10.1109/AIoTC63215.2024.10748289},
}

@INPROCEEDINGS{10983758,
  author =       {Al Azri, Kawther Zahir and Tumati, Raja and Al
                  Tarshi, Shahd Saud},
  booktitle =    {2025 International Conference for Artificial
                  Intelligence, Applications, Innovation and Ethics
                  (AI2E)},
  title =        {Impact of Artificial Intelligence on Student
                  Learning in Oman},
  year =         2025,
  pages =        {1-5},
  keywords =     {Surveys;Technological
                  innovation;Ethics;Education;Learning (artificial
                  intelligence);Chatbots;Internet;Artificial
                  intelligence tools;student learning;AI benefits;AI
                  challenges;AI education},
  doi =          {10.1109/AI2E64943.2025.10983758},
}

@article{jensen_generative_2025,
  title =        {Generative {AI} and higher education: a review of
                  claims from the first months of {ChatGPT}},
  volume =       89,
  issn =         {1573-174X},
  shorttitle =   {Generative {AI} and higher education},
  url =          {https://doi.org/10.1007/s10734-024-01265-3},
  doi =          {10.1007/s10734-024-01265-3},
  abstract =     {The release of the Artificial Intelligence (AI)
                  chatbot ChatGPT renewed discussions about how AI
                  would upend higher education. This paper presents a
                  critical analysis of “grey literature” claims made
                  in the first months after ChatGPT was made public,
                  exploring what these discussions might mobilise in
                  practice. We identified articles for inclusion
                  through a systematic search of five prominent higher
                  education sector outlets. The included articles were
                  thematically coded for claims about generative AI
                  and higher education. We identified ten claims:
                  Three about the nature of ChatGPT, four about
                  changing practices of institutions and teachers, and
                  three about new independent practices of students.
                  Overall, the claims present a positive perspective
                  on AI in higher education. While being perceived as
                  a disruption of the status quo, the authors
                  generally frame AI as a catalyst for existing
                  agendas, e.g. assessment reform, personalisation, or
                  inclusion. This suggests a focus on embracing the
                  affordances offered by AI and primarily addressing
                  risks by including AI in curricula. Furthermore, the
                  claims mainly portray students as either plagiarists
                  or victims of a failing educational system. The
                  paper proposes that a more critical interrogation of
                  generative AI, and the involvement of students in
                  the conversation, may be beneficial.},
  language =     {en},
  number =       4,
  urldate =      {2025-11-21},
  journal =      {Higher Education},
  author =       {Jensen, Lasse X. and Buhl, Alexandra and Sharma,
                  Anjali and Bearman, Margaret},
  month =        apr,
  year =         2025,
  keywords =     {ChatGPT, Artificial Intelligence, Generative AI,
                  Higher education, Grey literature, Scoping review},
  pages =        {1145--1161},
}

@inproceedings{gan_large_2023,
  title =        {Large {Language} {Models} in {Education}: {Vision}
                  and {Opportunities}},
  shorttitle =   {Large {Language} {Models} in {Education}},
  url =
                  {https://ieeexplore.ieee.org/abstract/document/10386291},
  doi =          {10.1109/BigData59044.2023.10386291},
  abstract =     {With the rapid development of artificial
                  intelligence technology, large language models
                  (LLMs) have become a hot research topic. Education
                  plays an important role in human social development
                  and progress. Traditional education faces challenges
                  such as individual student differences, insufficient
                  allocation of teaching resources, and assessment of
                  teaching effectiveness. Therefore, the applications
                  of LLMs in the field of digital/smart education have
                  broad prospects. The research on educational large
                  models (EduLLMs) is constantly evolving, providing
                  new methods and approaches to achieve personalized
                  learning, intelligent tutoring, and educational
                  assessment goals, thereby improving the quality of
                  education and the learning experience. This article
                  aims to investigate and summarize the application of
                  LLMs in smart education. It first introduces the
                  research background and motivation of LLMs and
                  explains the essence of LLMs. It then discusses the
                  relationship between digital education and EduLLMs
                  and summarizes the current research status of
                  educational large models. The main contributions are
                  the systematic summary and vision of the research
                  background, motivation, and application of large
                  models for education (LLM4Edu). By reviewing
                  existing research, this article provides guidance
                  and insights for educators, researchers, and
                  policy-makers to gain a deep understanding of the
                  potential and challenges of LLM4Edu. It further
                  provides guidance for further advancing the
                  development and application of LLM4Edu, while still
                  facing technical, ethical, and practical challenges
                  requiring further research and exploration.},
  urldate =      {2025-11-21},
  booktitle =    {2023 {IEEE} {International} {Conference} on {Big}
                  {Data} ({BigData})},
  author =       {Gan, Wensheng and Qi, Zhenlian and Wu, Jiayang and
                  Lin, Jerry Chun-Wei},
  month =        dec,
  year =         2023,
  keywords =     {Ethics, Analytical models, Systematics, Machine
                  vision, Education, Educational technology, Big Data,
                  artificial intelligence, LLMs, smart education,
                  vision, opportunities},
  pages =        {4776--4785},
}

@book{russell_artificial_2021,
  title =        {Artificial Intelligence: A Modern Approach},
  edition =      {4th},
  publisher =    {Pearson},
  author =       {Russell, Stuart J. and Norvig, Peter},
  year =         2021,
  address =      {Hoboken},
}

@article{mccarthy_proposal_2006,
  title =        {A Proposal for the Dartmouth Summer Research Project
                  on Artificial Intelligence, August 31, 1955},
  volume =       27,
  number =       4,
  journal =      {AI Magazine},
  author =       {McCarthy, John and Minsky, Marvin and Rochester,
                  Nathaniel and Shannon, Claude},
  year =         2006,
  pages =        {12--14},
  note =         {Republicação do documento original de 1955},
}

@article{turing_computing_1950,
  title =        {Computing Machinery and Intelligence},
  volume =       59,
  number =       236,
  journal =      {Mind},
  author =       {Turing, A. M.},
  year =         1950,
  pages =        {433--460},
}

@book{mitchell_machine_1997,
  title =        {Machine Learning},
  publisher =    {McGraw-Hill},
  author =       {Mitchell, Tom M.},
  year =         1997,
  address =      {New York},
}

@online{unesco_guidance_2023,
  title =        {Guidance for generative AI in education and
                  research},
  url =          {https://unesdoc.unesco.org/ark:/48223/pf0000386693},
  author =       {{UNESCO}},
  year =         {2023},
  urldate =      {2025-11-21},
  organization = {United Nations Educational, Scientific and Cultural
                  Organization},
}

@book{goodfellow_deep_2016,
  title =        {Deep Learning},
  author =       {Goodfellow, Ian and Bengio, Yoshua and Courville,
                  Aaron},
  publisher =    {MIT Press},
  note =         {http://www.deeplearningbook.org},
  year =         2016,
  address =      {Cambridge, MA},
}

@article{lecun_deep_2015,
  title =        {Deep learning},
  volume =       521,
  number =       7553,
  journal =      {Nature},
  author =       {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year =         2015,
  pages =        {436--444},
  publisher =    {Nature Publishing Group},
}

@article{mcculloch_logical_1943,
  title =        {A logical calculus of the ideas immanent in nervous
                  activity},
  volume =       5,
  number =       4,
  journal =      {Bulletin of mathematical biophysics},
  author =       {McCulloch, Warren S. and Pitts, Walter},
  year =         1943,
  pages =        {115--133},
}

@inproceedings{vaswani_attention_2017,
  title =        {Attention is All You Need},
  booktitle =    {Advances in Neural Information Processing Systems},
  author =       {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki
                  and Uszkoreit, Jakob and Jones, Llion and Gomez,
                  Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  volume =       30,
  year =         2017,
  publisher =    {Curran Associates, Inc.},
  url =
                  {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
}

@book{jurafsky_speech_2024,
  title =        {Speech and Language Processing: An Introduction to
                  Natural Language Processing, Computational
                  Linguistics, and Speech Recognition},
  author =       {Jurafsky, Daniel and Martin, James H.},
  edition =      {3rd draft},
  year =         2024,
  publisher =    {Online draft},
  url =          {https://web.stanford.edu/~jurafsky/slp3/},
}

@article{brown_language_2020,
  title =        {Language Models are Few-Shot Learners},
  author =       {Brown, Tom and Mann, Benjamin and Ryder, Nick and
                  Subbiah, Melanie and Kaplan, Jared and Dhariwal,
                  Prafulla and Neelakantan, Arvind and Shyam, Pranav
                  and Sastry, Girish and Askell, Amanda},
  journal =      {Advances in Neural Information Processing Systems},
  volume =       33,
  pages =        {1877--1901},
  year =         2020,
  url =
                  {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
}

@article{ouyang_training_2022,
  title =        {Training language models to follow instructions with
                  human feedback},
  author =       {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and
                  Almeida, Diogo and Wainwright, Carroll and Mishkin,
                  Pamela and Zhang, Chong and Agarwal, Sandhini and
                  Slama, Katarina and Ray, Alex},
  journal =      {Advances in Neural Information Processing Systems},
  volume =       35,
  pages =        {27730--27744},
  year =         2022,
  url =
                  {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a0017cc-Paper.pdf},
}

@article{zhao_survey_2023,
  title =        {A Survey of Large Language Models},
  author =       {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and
                  Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and
                  Min, Yingqian and Zhang, Beichen and Zhang, Junjie
                  and Dong, Zican},
  journal =      {arXiv preprint arXiv:2303.18223},
  year =         2023,
  url =          {https://arxiv.org/abs/2303.18223},
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title =        {Learning Representations by Back-Propagating Errors},
  author =       {Rumelhart, David E. and Hinton, Geoffrey E. and
                  Williams, Ronald J.},
  date =         {1986-10},
  journaltitle = {Nature},
  volume =       {323},
  number =       {6088},
  pages =        {533--536},
  publisher =    {Nature Publishing Group},
  issn =         {1476-4687},
  doi =          {10.1038/323533a0},
  url =          {https://www.nature.com/articles/323533a0},
  urldate =      {2025-12-04},
  abstract =     {We describe a new learning procedure,
                  back-propagation, for networks of neurone-like
                  units. The procedure repeatedly adjusts the weights
                  of the connections in the network so as to minimize
                  a measure of the difference between the actual
                  output vector of the net and the desired output
                  vector. As a result of the weight adjustments,
                  internal ‘hidden’ units which are not part of the
                  input or output come to represent important features
                  of the task domain, and the regularities in the task
                  are captured by the interactions of these units. The
                  ability to create useful new features distinguishes
                  back-propagation from earlier, simpler methods such
                  as the perceptron-convergence procedure1.},
  langid =       {english},
  keywords =     {Humanities and Social
                  Sciences,multidisciplinary,Science},
  year =         {1986}
}

@online{raffelExploringLimitsTransfer2019,
  title =        {Exploring the {{Limits}} of {{Transfer Learning}}
                  with a {{Unified Text-to-Text Transformer}}},
  author =       {Raffel, Colin and Shazeer, Noam and Roberts, Adam
                  and Lee, Katherine and Narang, Sharan and Matena,
                  Michael and Zhou, Yanqi and Li, Wei and Liu, Peter
                  J.},
  date =         {2019-10-23},
  url =          {https://arxiv.org/abs/1910.10683v4},
  urldate =      {2025-12-04},
  abstract =     {Transfer learning, where a model is first
                  pre-trained on a data-rich task before being
                  fine-tuned on a downstream task, has emerged as a
                  powerful technique in natural language processing
                  (NLP). The effectiveness of transfer learning has
                  given rise to a diversity of approaches,
                  methodology, and practice. In this paper, we explore
                  the landscape of transfer learning techniques for
                  NLP by introducing a unified framework that converts
                  all text-based language problems into a text-to-text
                  format. Our systematic study compares pre-training
                  objectives, architectures, unlabeled data sets,
                  transfer approaches, and other factors on dozens of
                  language understanding tasks. By combining the
                  insights from our exploration with scale and our new
                  ``Colossal Clean Crawled Corpus'', we achieve
                  state-of-the-art results on many benchmarks covering
                  summarization, question answering, text
                  classification, and more. To facilitate future work
                  on transfer learning for NLP, we release our data
                  set, pre-trained models, and code.},
  langid =       {english},
  organization = {arXiv.org},
}

@article{turingComputableNumbersApplication1937,
  title =        {On {{Computable Numbers}}, with an {{Application}}
                  to the {{Entscheidungsproblem}}},
  author =       {Turing, A. M.},
  date =         {1937},
  journaltitle = {Proceedings of the London Mathematical Society},
  volume =       {s2-42},
  number =       {1},
  pages =        {230--265},
  issn =         {1460-244X},
  doi =          {10.1112/plms/s2-42.1.230},
  url =
                  {https://onlinelibrary.wiley.com/doi/abs/10.1112/plms/s2-42.1.230},
  urldate =      {2025-12-13},
  langid =       {english},
  year =         {1937}
}
@article{hochreiter_long_1997,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997}
}

@inproceedings{strubell_energy_2019,
  title={Energy and Policy Considerations for Deep Learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3645--3650},
  year={2019}
}

@article{bengio_learning_1994,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994}
}
